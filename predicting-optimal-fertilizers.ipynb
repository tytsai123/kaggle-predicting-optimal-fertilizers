{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":91717,"databundleVersionId":12184666,"sourceType":"competition"}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Load Data","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd \nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-07-01T16:27:33.875944Z","iopub.execute_input":"2025-07-01T16:27:33.876184Z","iopub.status.idle":"2025-07-01T16:27:34.210937Z","shell.execute_reply.started":"2025-07-01T16:27:33.876159Z","shell.execute_reply":"2025-07-01T16:27:34.210107Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/playground-series-s5e6/sample_submission.csv\n/kaggle/input/playground-series-s5e6/train.csv\n/kaggle/input/playground-series-s5e6/test.csv\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"train_df = pd.read_csv(\"/kaggle/input/playground-series-s5e6/train.csv\")\ntest_df = pd.read_csv(\"/kaggle/input/playground-series-s5e6/test.csv\")\nsample_sub_df = pd.read_csv(\"/kaggle/input/playground-series-s5e6/sample_submission.csv\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-01T16:27:34.212839Z","iopub.execute_input":"2025-07-01T16:27:34.213134Z","iopub.status.idle":"2025-07-01T16:27:35.185195Z","shell.execute_reply.started":"2025-07-01T16:27:34.213115Z","shell.execute_reply":"2025-07-01T16:27:35.184616Z"}},"outputs":[],"execution_count":2},{"cell_type":"markdown","source":"# Exploratory Data Analysis","metadata":{}},{"cell_type":"code","source":"print(\"Original Number of Rows: \", len(train_df))\ntrain_df = train_df.dropna()\nprint(\"Filtered Number of Rows: \", len(train_df))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-01T16:27:35.185891Z","iopub.execute_input":"2025-07-01T16:27:35.186146Z","iopub.status.idle":"2025-07-01T16:27:35.314182Z","shell.execute_reply.started":"2025-07-01T16:27:35.186120Z","shell.execute_reply":"2025-07-01T16:27:35.313557Z"}},"outputs":[{"name":"stdout","text":"Original Number of Rows:  750000\nFiltered Number of Rows:  750000\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"train_df.groupby('Fertilizer Name')['id'].count()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-01T16:27:35.314871Z","iopub.execute_input":"2025-07-01T16:27:35.315051Z","iopub.status.idle":"2025-07-01T16:27:35.366117Z","shell.execute_reply.started":"2025-07-01T16:27:35.315035Z","shell.execute_reply":"2025-07-01T16:27:35.365532Z"}},"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"Fertilizer Name\n10-26-26    113887\n14-35-14    114436\n17-17-17    112453\n20-20       110889\n28-28       111158\nDAP          94860\nUrea         92317\nName: id, dtype: int64"},"metadata":{}}],"execution_count":4},{"cell_type":"code","source":"fertilizer_column = 'Fertilizer Name'\nnumerical_features = ['Temparature', 'Humidity', 'Moisture', 'Nitrogen', 'Potassium', 'Phosphorous']\nfor feature in numerical_features:\n    print(f\"\\n--- Distribution of '{feature}' for each '{fertilizer_column}' ---\")\n    distribution = train_df.groupby(fertilizer_column)[feature].agg(['mean', 'median', 'std', 'min', 'max', 'count'])\n    print(distribution)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-01T16:27:35.366890Z","iopub.execute_input":"2025-07-01T16:27:35.367145Z","iopub.status.idle":"2025-07-01T16:27:35.822387Z","shell.execute_reply.started":"2025-07-01T16:27:35.367126Z","shell.execute_reply":"2025-07-01T16:27:35.821730Z"}},"outputs":[{"name":"stdout","text":"\n--- Distribution of 'Temparature' for each 'Fertilizer Name' ---\n                      mean  median       std  min  max   count\nFertilizer Name                                               \n10-26-26         31.470589    32.0  4.038993   25   38  113887\n14-35-14         31.543247    32.0  4.001384   25   38  114436\n17-17-17         31.463803    31.0  4.030426   25   38  112453\n20-20            31.514406    31.0  4.025341   25   38  110889\n28-28            31.518973    32.0  4.029370   25   38  111158\nDAP              31.508065    32.0  4.051078   25   38   94860\nUrea             31.507296    32.0  4.001744   25   38   92317\n\n--- Distribution of 'Humidity' for each 'Fertilizer Name' ---\n                      mean  median       std  min  max   count\nFertilizer Name                                               \n10-26-26         60.937043    61.0  6.640019   50   72  113887\n14-35-14         60.962669    61.0  6.641917   50   72  114436\n17-17-17         60.998755    61.0  6.612568   50   72  112453\n20-20            61.104230    61.0  6.661199   50   72  110889\n28-28            61.013179    61.0  6.654917   50   72  111158\nDAP              61.155703    61.0  6.674935   50   72   94860\nUrea             61.140527    61.0  6.650299   50   72   92317\n\n--- Distribution of 'Moisture' for each 'Fertilizer Name' ---\n                      mean  median        std  min  max   count\nFertilizer Name                                                \n10-26-26         45.075856    45.0  11.786084   25   65  113887\n14-35-14         45.100047    45.0  11.786555   25   65  114436\n17-17-17         44.740345    45.0  11.760048   25   65  112453\n20-20            45.316713    45.0  11.769782   25   65  110889\n28-28            45.500297    46.0  11.853618   25   65  111158\nDAP              45.261944    46.0  11.858084   25   65   94860\nUrea             45.342743    46.0  11.731911   25   65   92317\n\n--- Distribution of 'Nitrogen' for each 'Fertilizer Name' ---\n                      mean  median        std  min  max   count\nFertilizer Name                                                \n10-26-26         23.202648    23.0  11.244531    4   42  113887\n14-35-14         23.083453    23.0  11.106016    4   42  114436\n17-17-17         23.205215    23.0  11.191768    4   42  112453\n20-20            23.008441    23.0  11.253946    4   42  110889\n28-28            23.013431    23.0  11.231735    4   42  111158\nDAP              23.155977    23.0  11.274307    4   42   94860\nUrea             22.972107    23.0  11.219700    4   42   92317\n\n--- Distribution of 'Potassium' for each 'Fertilizer Name' ---\n                     mean  median       std  min  max   count\nFertilizer Name                                              \n10-26-26         9.555138    10.0  5.787646    0   19  113887\n14-35-14         9.484568     9.0  5.728002    0   19  114436\n17-17-17         9.523899    10.0  5.777925    0   19  112453\n20-20            9.590329    10.0  5.812375    0   19  110889\n28-28            9.478220     9.0  5.715701    0   19  111158\nDAP              9.252878     9.0  5.768208    0   19   94860\nUrea             9.417323     9.0  5.763855    0   19   92317\n\n--- Distribution of 'Phosphorous' for each 'Fertilizer Name' ---\n                      mean  median        std  min  max   count\nFertilizer Name                                                \n10-26-26         20.980885    21.0  12.388746    0   42  113887\n14-35-14         21.410221    22.0  12.375613    0   42  114436\n17-17-17         21.053934    21.0  12.340902    0   42  112453\n20-20            21.110633    21.0  12.320987    0   42  110889\n28-28            21.024821    21.0  12.397456    0   42  111158\nDAP              20.956051    21.0  12.293027    0   42   94860\nUrea             20.926666    21.0  12.284100    0   42   92317\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"train_df['P/N Ratio'] = train_df['Potassium'] / train_df['Nitrogen']\ntrain_df['K/N Ratio'] = train_df['Phosphorous'] / train_df['Nitrogen']\ntrain_df['Env Max'] = train_df[['Temparature', 'Humidity', 'Moisture']].max(axis=1)\nsynthetic_features = ['P/N Ratio', 'K/N Ratio', 'Env Max']\n\nfor feature in synthetic_features:\n    print(f\"\\n--- Distribution of '{feature}' for each '{fertilizer_column}' ---\")\n    train_df[feature] = train_df[feature].replace([np.inf, -np.inf], np.nan)\n    distribution = train_df.groupby(fertilizer_column)[feature].agg(['mean', 'median', 'std', 'min', 'max', 'count'])\n    print(distribution)\n    \ntest_df['P/N Ratio'] = test_df['Potassium'] / test_df['Nitrogen']\ntest_df['K/N Ratio'] = test_df['Phosphorous'] / test_df['Nitrogen']\ntest_df['Env Max'] = test_df[['Temparature', 'Humidity', 'Moisture']].max(axis=1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-01T16:27:35.823085Z","iopub.execute_input":"2025-07-01T16:27:35.823293Z","iopub.status.idle":"2025-07-01T16:27:36.195801Z","shell.execute_reply.started":"2025-07-01T16:27:35.823275Z","shell.execute_reply":"2025-07-01T16:27:36.195166Z"}},"outputs":[{"name":"stdout","text":"\n--- Distribution of 'P/N Ratio' for each 'Fertilizer Name' ---\n                     mean    median       std  min   max   count\nFertilizer Name                                                 \n10-26-26         0.601993  0.411765  0.671760  0.0  4.75  113887\n14-35-14         0.595436  0.411765  0.657911  0.0  4.75  114436\n17-17-17         0.596140  0.409091  0.656708  0.0  4.75  112453\n20-20            0.609874  0.416667  0.674232  0.0  4.75  110889\n28-28            0.605453  0.411765  0.671077  0.0  4.75  111158\nDAP              0.590018  0.400000  0.669557  0.0  4.75   94860\nUrea             0.605761  0.407407  0.681578  0.0  4.75   92317\n\n--- Distribution of 'K/N Ratio' for each 'Fertilizer Name' ---\n                     mean    median       std  min   max   count\nFertilizer Name                                                 \n10-26-26         1.337029  0.903226  1.493318  0.0  10.5  113887\n14-35-14         1.347489  0.926829  1.465407  0.0  10.5  114436\n17-17-17         1.318852  0.900000  1.429726  0.0  10.5  112453\n20-20            1.347051  0.923077  1.464690  0.0  10.5  110889\n28-28            1.349713  0.911765  1.483289  0.0  10.5  111158\nDAP              1.334404  0.903226  1.472483  0.0  10.5   94860\nUrea             1.341706  0.911765  1.466211  0.0  10.5   92317\n\n--- Distribution of 'Env Max' for each 'Fertilizer Name' ---\n                      mean  median       std  min  max   count\nFertilizer Name                                               \n10-26-26         61.661726    62.0  6.208473   50   72  113887\n14-35-14         61.688105    62.0  6.206047   50   72  114436\n17-17-17         61.695393    62.0  6.194001   50   72  112453\n20-20            61.824212    62.0  6.216300   50   72  110889\n28-28            61.781968    62.0  6.196925   50   72  111158\nDAP              61.877187    62.0  6.230555   50   72   94860\nUrea             61.866937    62.0  6.203508   50   72   92317\n","output_type":"stream"}],"execution_count":6},{"cell_type":"markdown","source":"## Mutual Infromation","metadata":{}},{"cell_type":"code","source":"from sklearn.feature_selection import mutual_info_classif\nfrom sklearn.preprocessing import LabelEncoder\n\nX = train_df.drop(columns=['Fertilizer Name'])\ny = train_df['Fertilizer Name']\n# Identify categorical and numerical columns in X\ncategorical_cols_X = X.select_dtypes(include=['object']).columns\nnumerical_cols_X = X.select_dtypes(include=[np.number]).columns\n\n# Apply Label Encoding to categorical features in X and the target variable y\nle = LabelEncoder()\nfor col in categorical_cols_X:\n    X[col] = le.fit_transform(X[col])\ny_encoded = le.fit_transform(y)\n\n# Create a boolean mask for discrete features for mutual_info_classif\ndiscrete_features_mask = np.array([col in categorical_cols_X for col in X.columns])\n\n# Calculate mutual information scores\nmi_scores = mutual_info_classif(X, y_encoded, discrete_features=discrete_features_mask, random_state=42)\n\n# Create a Series for better readability and sort them\nmi_series = pd.Series(mi_scores, index=X.columns)\nmi_series = mi_series.sort_values(ascending=False)\n\nprint(\"Mutual Information Scores with respect to 'Fertilizer Name':\")\nprint(mi_series)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-01T16:27:36.196473Z","iopub.execute_input":"2025-07-01T16:27:36.196662Z","iopub.status.idle":"2025-07-01T16:28:24.406607Z","shell.execute_reply.started":"2025-07-01T16:27:36.196645Z","shell.execute_reply":"2025-07-01T16:28:24.405801Z"}},"outputs":[{"name":"stdout","text":"Mutual Information Scores with respect to 'Fertilizer Name':\nK/N Ratio      0.008024\nP/N Ratio      0.003845\nPhosphorous    0.003201\nMoisture       0.002858\nEnv Max        0.002267\nNitrogen       0.002263\nHumidity       0.002151\nCrop Type      0.002088\nPotassium      0.002057\nTemparature    0.001872\nSoil Type      0.000617\nid             0.000000\ndtype: float64\n","output_type":"stream"}],"execution_count":7},{"cell_type":"markdown","source":"# Training Model","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder\nfrom sklearn.compose import ColumnTransformer\n# Define numerical features and target variable\nnumerical_features = ['Temparature', 'Humidity', 'Moisture', 'Nitrogen', 'Potassium', 'Phosphorous']\ncategorical_features = ['Soil Type', 'Crop Type']\nsynthetic_features = ['P/N Ratio', 'K/N Ratio', 'Env Max']\ntarget = 'Fertilizer Name'\n\n# Extract features (X) and target (y)\nX = train_df[numerical_features + categorical_features].copy()\ny = train_df[target]\n\n# add rank\nfor col in numerical_features:\n    X.loc[:, f'{col}_Rank'] = X[col].rank(method='average', ascending=True)\n    test_df.loc[:, f'{col}_Rank'] = test_df[col].rank(method='average', ascending=True)\n\n# Preprocessing for numerical and categorical features\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('num', StandardScaler(), numerical_features),\n        ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False), categorical_features)\n    ],\n    remainder='drop'\n)\n\n# Apply preprocessing to X\nX_train = preprocessor.fit_transform(X)\nX_test = preprocessor.transform(test_df[numerical_features + categorical_features])\n\n# Encode the target variable y using LabelEncoder\nlabel_encoder = LabelEncoder()\ny_encoded = label_encoder.fit_transform(y)\nfertilizer_classes = label_encoder.classes_\nprint(\"Fertilizer classes (order):\", fertilizer_classes)\n\nX_train = pd.DataFrame(np.array(X_train))\nX_test = pd.DataFrame(np.array(X_test))\ny_encoded = pd.DataFrame(np.array(y_encoded))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-01T16:28:24.409237Z","iopub.execute_input":"2025-07-01T16:28:24.409587Z","iopub.status.idle":"2025-07-01T16:28:25.942055Z","shell.execute_reply.started":"2025-07-01T16:28:24.409565Z","shell.execute_reply":"2025-07-01T16:28:25.941347Z"}},"outputs":[{"name":"stdout","text":"Fertilizer classes (order): ['10-26-26' '14-35-14' '17-17-17' '20-20' '28-28' 'DAP' 'Urea']\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"X_train = pd.DataFrame(np.array(X_train))\nX_test = pd.DataFrame(np.array(X_test))\ny_encoded = pd.DataFrame(np.array(y_encoded))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-01T16:28:25.943057Z","iopub.execute_input":"2025-07-01T16:28:25.943635Z","iopub.status.idle":"2025-07-01T16:28:26.003955Z","shell.execute_reply.started":"2025-07-01T16:28:25.943608Z","shell.execute_reply":"2025-07-01T16:28:26.003348Z"}},"outputs":[],"execution_count":9},{"cell_type":"markdown","source":"## Metric: Mean Average Precision @ 3 (MAP@3)","metadata":{}},{"cell_type":"code","source":"def mapk(actual, predicted, k=3):\n    return np.mean([apk(a, p, k) for a, p in zip(actual, predicted)])\n\ndef apk(actual, predicted, k=3):\n    if len(predicted) > k:\n        predicted = predicted[:k]\n\n    score = 0.0\n    num_hits = 0.0\n\n    for i, p in enumerate(predicted):\n        if p in actual and p not in predicted[:i]:\n            num_hits += 1.0\n            score += num_hits / (i + 1.0)\n\n    if not actual:\n        return 0.0\n\n    return score / min(len(actual), k)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-01T16:28:26.004651Z","iopub.execute_input":"2025-07-01T16:28:26.004895Z","iopub.status.idle":"2025-07-01T16:28:26.010752Z","shell.execute_reply.started":"2025-07-01T16:28:26.004877Z","shell.execute_reply":"2025-07-01T16:28:26.009931Z"}},"outputs":[],"execution_count":10},{"cell_type":"markdown","source":"## Base Model: XGBoost & LightGBM","metadata":{}},{"cell_type":"code","source":"import xgboost as xgb\nfrom xgboost import XGBClassifier\nfrom xgboost.callback import EarlyStopping\nimport lightgbm as lgb\nfrom lightgbm import LGBMRegressor, LGBMClassifier, log_evaluation, early_stopping\n\nmodel_configs = {\n    'xgb': {'model': XGBClassifier, 'params': {'objective': 'multi:softprob', \n                                               'num_class': y.nunique(), \n                                               'max_depth': 7, \n                                               'learning_rate': 0.03, \n                                               'subsample': 0.8, \n                                               'colsample_bytree': 0.4, \n                                               'tree_method': 'hist', \n                                               'random_state': 42, \n                                               'eval_metric': 'mlogloss', \n                                               'device': \"cuda\", \n                                               'n_estimators':10000,\n                                               'early_stopping_rounds': 100}},\n    'lgb': {'model': LGBMClassifier, 'params': {'objective': 'multiclass', \n                                                'num_class': y.nunique(), \n                                                \"device\": \"gpu\", \n                                                \"learning_rate\": 0.03, \n                                                \"max_depth\": 7, \n                                                \"n_estimators\": 10000, \n                                                \"n_jobs\": -1, \n                                                \"num_leaves\": 31, \n                                                \"random_state\": 42, \n                                                \"verbose\": -1}}\n}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-01T16:28:26.011729Z","iopub.execute_input":"2025-07-01T16:28:26.012086Z","iopub.status.idle":"2025-07-01T16:28:27.442127Z","shell.execute_reply.started":"2025-07-01T16:28:26.012070Z","shell.execute_reply":"2025-07-01T16:28:27.441323Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"import warnings\n\n# Suppress warnings as before\nwith warnings.catch_warnings():\n    warnings.simplefilter(\"ignore\", category=UserWarning)\n    warnings.simplefilter(\"ignore\", category=pd.errors.SettingWithCopyWarning)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-01T16:28:27.443108Z","iopub.execute_input":"2025-07-01T16:28:27.444075Z","iopub.status.idle":"2025-07-01T16:28:27.447886Z","shell.execute_reply.started":"2025-07-01T16:28:27.444053Z","shell.execute_reply":"2025-07-01T16:28:27.447044Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"from sklearn.model_selection import StratifiedKFold\n\nnum_of_folds = 10\nskf = StratifiedKFold(n_splits=num_of_folds, shuffle=True, random_state=42)\n\noof_preds = {name: np.zeros((len(X_train), y.nunique())) for name in model_configs}\ntest_preds = {name: np.zeros((len(X_test), y.nunique())) for name in model_configs}\n\nfor name, config in model_configs.items():\n    print(f\"\\n Training Base Model: {name.upper()}\")\n    current_model_test_preds = np.zeros((len(X_test), y.nunique()))\n\n    for fold, (train_idx, valid_idx) in enumerate(skf.split(X_train, y_encoded)):\n        print(f\"  Fold {fold + 1}/{num_of_folds}\")\n\n        x_train_fit = X_train.iloc[train_idx].values\n        x_valid_fit = X_train.iloc[valid_idx].values\n        y_train_fit = y_encoded.iloc[train_idx].values\n        y_valid_fit = y_encoded.iloc[valid_idx].values\n        \n        model = config['model'](**config['params'])\n\n        if name == 'xgb':\n            model.fit(x_train_fit, y_train_fit,\n                      eval_set=[(x_train_fit, y_train_fit), (x_valid_fit, y_valid_fit)],\n                      verbose=100)\n            \n            if hasattr(model, 'evals_result_') and model.evals_result_:\n                print(f\"    XGBoost Fold {fold+1} Final Train Loss: {model.evals_result_['validation_0']['mlogloss'][-1]:.4f}\")\n                print(f\"    XGBoost Fold {fold+1} Final Valid Loss: {model.evals_result_['validation_1']['mlogloss'][-1]:.4f}\")\n            else:\n                print(f\"    XGBoost Fold {fold+1}: No evaluation results found (check eval_set or verbose).\")\n\n        else: # For LightGBM\n            # eval_set data is now also simply NumPy arrays\n            eval_set_lgbm = [\n                (x_train_fit, y_train_fit),\n                (x_valid_fit, y_valid_fit)\n            ]\n            callbacks = [lgb.early_stopping(100, verbose=False), lgb.log_evaluation(period=100)]\n\n            model.fit(x_train_fit, y_train_fit,\n                      eval_set=eval_set_lgbm,\n                      callbacks=callbacks)\n\n            if model.best_iteration_:\n                print(f\"    LightGBM Fold {fold+1} Best Iteration: {model.best_iteration_}\")\n                print(f\"    LightGBM Fold {fold+1} Final Valid Loss (Best Score): {model.best_score_['valid_1']['multi_logloss']:.4f}\")\n            else:\n                print(f\"    LightGBM Fold {fold+1} Final Valid Loss: {model.evals_result_['valid_1']['multi_logloss'][-1]:.4f}\")\n        \n        valid_preds = model.predict_proba(x_valid_fit)\n        test_preds_fold = model.predict_proba(X_test)\n\n        oof_preds[name][valid_idx] = valid_preds\n        current_model_test_preds += test_preds_fold / num_of_folds\n\n    test_preds[name] = current_model_test_preds\n\nprint(\"\\nBase model training complete for all configurations.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-01T16:28:27.448653Z","iopub.execute_input":"2025-07-01T16:28:27.448883Z","iopub.status.idle":"2025-07-01T18:35:38.646694Z","shell.execute_reply.started":"2025-07-01T16:28:27.448857Z","shell.execute_reply":"2025-07-01T18:35:38.645839Z"}},"outputs":[{"name":"stdout","text":"\n Training Base Model: XGB\n  Fold 1/10\n[0]\tvalidation_0-mlogloss:1.94563\tvalidation_1-mlogloss:1.94567\n[100]\tvalidation_0-mlogloss:1.92856\tvalidation_1-mlogloss:1.93376\n[200]\tvalidation_0-mlogloss:1.91833\tvalidation_1-mlogloss:1.92843\n[300]\tvalidation_0-mlogloss:1.91005\tvalidation_1-mlogloss:1.92474\n[400]\tvalidation_0-mlogloss:1.90298\tvalidation_1-mlogloss:1.92207\n[500]\tvalidation_0-mlogloss:1.89651\tvalidation_1-mlogloss:1.91973\n[600]\tvalidation_0-mlogloss:1.89054\tvalidation_1-mlogloss:1.91785\n[700]\tvalidation_0-mlogloss:1.88471\tvalidation_1-mlogloss:1.91623\n[800]\tvalidation_0-mlogloss:1.87916\tvalidation_1-mlogloss:1.91486\n[900]\tvalidation_0-mlogloss:1.87389\tvalidation_1-mlogloss:1.91369\n[1000]\tvalidation_0-mlogloss:1.86886\tvalidation_1-mlogloss:1.91255\n[1100]\tvalidation_0-mlogloss:1.86393\tvalidation_1-mlogloss:1.91164\n[1200]\tvalidation_0-mlogloss:1.85929\tvalidation_1-mlogloss:1.91081\n[1300]\tvalidation_0-mlogloss:1.85473\tvalidation_1-mlogloss:1.91007\n[1400]\tvalidation_0-mlogloss:1.85040\tvalidation_1-mlogloss:1.90938\n[1500]\tvalidation_0-mlogloss:1.84610\tvalidation_1-mlogloss:1.90878\n[1600]\tvalidation_0-mlogloss:1.84184\tvalidation_1-mlogloss:1.90829\n[1700]\tvalidation_0-mlogloss:1.83773\tvalidation_1-mlogloss:1.90783\n[1800]\tvalidation_0-mlogloss:1.83382\tvalidation_1-mlogloss:1.90741\n[1900]\tvalidation_0-mlogloss:1.82993\tvalidation_1-mlogloss:1.90702\n[2000]\tvalidation_0-mlogloss:1.82601\tvalidation_1-mlogloss:1.90665\n[2100]\tvalidation_0-mlogloss:1.82219\tvalidation_1-mlogloss:1.90636\n[2200]\tvalidation_0-mlogloss:1.81859\tvalidation_1-mlogloss:1.90610\n[2300]\tvalidation_0-mlogloss:1.81490\tvalidation_1-mlogloss:1.90585\n[2400]\tvalidation_0-mlogloss:1.81137\tvalidation_1-mlogloss:1.90558\n[2500]\tvalidation_0-mlogloss:1.80788\tvalidation_1-mlogloss:1.90547\n[2600]\tvalidation_0-mlogloss:1.80439\tvalidation_1-mlogloss:1.90535\n[2700]\tvalidation_0-mlogloss:1.80101\tvalidation_1-mlogloss:1.90522\n[2800]\tvalidation_0-mlogloss:1.79759\tvalidation_1-mlogloss:1.90512\n[2900]\tvalidation_0-mlogloss:1.79429\tvalidation_1-mlogloss:1.90509\n[3000]\tvalidation_0-mlogloss:1.79096\tvalidation_1-mlogloss:1.90503\n[3100]\tvalidation_0-mlogloss:1.78774\tvalidation_1-mlogloss:1.90496\n[3200]\tvalidation_0-mlogloss:1.78449\tvalidation_1-mlogloss:1.90495\n[3300]\tvalidation_0-mlogloss:1.78129\tvalidation_1-mlogloss:1.90490\n[3400]\tvalidation_0-mlogloss:1.77826\tvalidation_1-mlogloss:1.90494\n[3415]\tvalidation_0-mlogloss:1.77781\tvalidation_1-mlogloss:1.90495\n    XGBoost Fold 1 Final Train Loss: 1.7778\n    XGBoost Fold 1 Final Valid Loss: 1.9049\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/xgboost/core.py:160: UserWarning: [16:31:38] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.\nPotential solutions:\n- Use a data structure that matches the device ordinal in the booster.\n- Set the device for booster before call to inplace_predict.\n\nThis warning will only be shown once.\n\n  warnings.warn(smsg, UserWarning)\n","output_type":"stream"},{"name":"stdout","text":"  Fold 2/10\n[0]\tvalidation_0-mlogloss:1.94564\tvalidation_1-mlogloss:1.94567\n[100]\tvalidation_0-mlogloss:1.92857\tvalidation_1-mlogloss:1.93361\n[200]\tvalidation_0-mlogloss:1.91826\tvalidation_1-mlogloss:1.92809\n[300]\tvalidation_0-mlogloss:1.91007\tvalidation_1-mlogloss:1.92440\n[400]\tvalidation_0-mlogloss:1.90300\tvalidation_1-mlogloss:1.92160\n[500]\tvalidation_0-mlogloss:1.89661\tvalidation_1-mlogloss:1.91938\n[600]\tvalidation_0-mlogloss:1.89061\tvalidation_1-mlogloss:1.91750\n[700]\tvalidation_0-mlogloss:1.88476\tvalidation_1-mlogloss:1.91583\n[800]\tvalidation_0-mlogloss:1.87924\tvalidation_1-mlogloss:1.91438\n[900]\tvalidation_0-mlogloss:1.87396\tvalidation_1-mlogloss:1.91313\n[1000]\tvalidation_0-mlogloss:1.86897\tvalidation_1-mlogloss:1.91210\n[1100]\tvalidation_0-mlogloss:1.86406\tvalidation_1-mlogloss:1.91104\n[1200]\tvalidation_0-mlogloss:1.85944\tvalidation_1-mlogloss:1.91010\n[1300]\tvalidation_0-mlogloss:1.85486\tvalidation_1-mlogloss:1.90926\n[1400]\tvalidation_0-mlogloss:1.85051\tvalidation_1-mlogloss:1.90852\n[1500]\tvalidation_0-mlogloss:1.84623\tvalidation_1-mlogloss:1.90790\n[1600]\tvalidation_0-mlogloss:1.84200\tvalidation_1-mlogloss:1.90731\n[1700]\tvalidation_0-mlogloss:1.83787\tvalidation_1-mlogloss:1.90680\n[1800]\tvalidation_0-mlogloss:1.83395\tvalidation_1-mlogloss:1.90636\n[1900]\tvalidation_0-mlogloss:1.83005\tvalidation_1-mlogloss:1.90597\n[2000]\tvalidation_0-mlogloss:1.82614\tvalidation_1-mlogloss:1.90563\n[2100]\tvalidation_0-mlogloss:1.82234\tvalidation_1-mlogloss:1.90528\n[2200]\tvalidation_0-mlogloss:1.81876\tvalidation_1-mlogloss:1.90497\n[2300]\tvalidation_0-mlogloss:1.81509\tvalidation_1-mlogloss:1.90474\n[2400]\tvalidation_0-mlogloss:1.81155\tvalidation_1-mlogloss:1.90448\n[2500]\tvalidation_0-mlogloss:1.80804\tvalidation_1-mlogloss:1.90429\n[2600]\tvalidation_0-mlogloss:1.80455\tvalidation_1-mlogloss:1.90413\n[2700]\tvalidation_0-mlogloss:1.80121\tvalidation_1-mlogloss:1.90398\n[2800]\tvalidation_0-mlogloss:1.79779\tvalidation_1-mlogloss:1.90383\n[2900]\tvalidation_0-mlogloss:1.79451\tvalidation_1-mlogloss:1.90371\n[3000]\tvalidation_0-mlogloss:1.79115\tvalidation_1-mlogloss:1.90363\n[3100]\tvalidation_0-mlogloss:1.78795\tvalidation_1-mlogloss:1.90357\n[3200]\tvalidation_0-mlogloss:1.78473\tvalidation_1-mlogloss:1.90355\n[3300]\tvalidation_0-mlogloss:1.78153\tvalidation_1-mlogloss:1.90348\n[3400]\tvalidation_0-mlogloss:1.77855\tvalidation_1-mlogloss:1.90348\n[3421]\tvalidation_0-mlogloss:1.77794\tvalidation_1-mlogloss:1.90347\n    XGBoost Fold 2 Final Train Loss: 1.7779\n    XGBoost Fold 2 Final Valid Loss: 1.9035\n  Fold 3/10\n[0]\tvalidation_0-mlogloss:1.94564\tvalidation_1-mlogloss:1.94568\n[100]\tvalidation_0-mlogloss:1.92861\tvalidation_1-mlogloss:1.93364\n[200]\tvalidation_0-mlogloss:1.91837\tvalidation_1-mlogloss:1.92825\n[300]\tvalidation_0-mlogloss:1.91016\tvalidation_1-mlogloss:1.92463\n[400]\tvalidation_0-mlogloss:1.90310\tvalidation_1-mlogloss:1.92190\n[500]\tvalidation_0-mlogloss:1.89666\tvalidation_1-mlogloss:1.91965\n[600]\tvalidation_0-mlogloss:1.89063\tvalidation_1-mlogloss:1.91776\n[700]\tvalidation_0-mlogloss:1.88483\tvalidation_1-mlogloss:1.91608\n[800]\tvalidation_0-mlogloss:1.87934\tvalidation_1-mlogloss:1.91462\n[900]\tvalidation_0-mlogloss:1.87404\tvalidation_1-mlogloss:1.91331\n[1000]\tvalidation_0-mlogloss:1.86903\tvalidation_1-mlogloss:1.91218\n[1100]\tvalidation_0-mlogloss:1.86413\tvalidation_1-mlogloss:1.91127\n[1200]\tvalidation_0-mlogloss:1.85949\tvalidation_1-mlogloss:1.91041\n[1300]\tvalidation_0-mlogloss:1.85495\tvalidation_1-mlogloss:1.90956\n[1400]\tvalidation_0-mlogloss:1.85061\tvalidation_1-mlogloss:1.90883\n[1500]\tvalidation_0-mlogloss:1.84631\tvalidation_1-mlogloss:1.90814\n[1600]\tvalidation_0-mlogloss:1.84204\tvalidation_1-mlogloss:1.90753\n[1700]\tvalidation_0-mlogloss:1.83793\tvalidation_1-mlogloss:1.90697\n[1800]\tvalidation_0-mlogloss:1.83404\tvalidation_1-mlogloss:1.90653\n[1900]\tvalidation_0-mlogloss:1.83012\tvalidation_1-mlogloss:1.90614\n[2000]\tvalidation_0-mlogloss:1.82617\tvalidation_1-mlogloss:1.90574\n[2100]\tvalidation_0-mlogloss:1.82234\tvalidation_1-mlogloss:1.90536\n[2200]\tvalidation_0-mlogloss:1.81875\tvalidation_1-mlogloss:1.90503\n[2300]\tvalidation_0-mlogloss:1.81505\tvalidation_1-mlogloss:1.90477\n[2400]\tvalidation_0-mlogloss:1.81155\tvalidation_1-mlogloss:1.90452\n[2500]\tvalidation_0-mlogloss:1.80805\tvalidation_1-mlogloss:1.90437\n[2600]\tvalidation_0-mlogloss:1.80459\tvalidation_1-mlogloss:1.90417\n[2700]\tvalidation_0-mlogloss:1.80124\tvalidation_1-mlogloss:1.90401\n[2800]\tvalidation_0-mlogloss:1.79785\tvalidation_1-mlogloss:1.90390\n[2900]\tvalidation_0-mlogloss:1.79455\tvalidation_1-mlogloss:1.90375\n[3000]\tvalidation_0-mlogloss:1.79120\tvalidation_1-mlogloss:1.90366\n[3100]\tvalidation_0-mlogloss:1.78800\tvalidation_1-mlogloss:1.90360\n[3200]\tvalidation_0-mlogloss:1.78476\tvalidation_1-mlogloss:1.90346\n[3300]\tvalidation_0-mlogloss:1.78155\tvalidation_1-mlogloss:1.90335\n[3400]\tvalidation_0-mlogloss:1.77857\tvalidation_1-mlogloss:1.90330\n[3500]\tvalidation_0-mlogloss:1.77554\tvalidation_1-mlogloss:1.90321\n[3600]\tvalidation_0-mlogloss:1.77256\tvalidation_1-mlogloss:1.90318\n[3700]\tvalidation_0-mlogloss:1.76961\tvalidation_1-mlogloss:1.90316\n[3800]\tvalidation_0-mlogloss:1.76665\tvalidation_1-mlogloss:1.90322\n    XGBoost Fold 3 Final Train Loss: 1.7666\n    XGBoost Fold 3 Final Valid Loss: 1.9032\n  Fold 4/10\n[0]\tvalidation_0-mlogloss:1.94564\tvalidation_1-mlogloss:1.94566\n[100]\tvalidation_0-mlogloss:1.92858\tvalidation_1-mlogloss:1.93351\n[200]\tvalidation_0-mlogloss:1.91839\tvalidation_1-mlogloss:1.92802\n[300]\tvalidation_0-mlogloss:1.91020\tvalidation_1-mlogloss:1.92428\n[400]\tvalidation_0-mlogloss:1.90309\tvalidation_1-mlogloss:1.92148\n[500]\tvalidation_0-mlogloss:1.89671\tvalidation_1-mlogloss:1.91919\n[600]\tvalidation_0-mlogloss:1.89068\tvalidation_1-mlogloss:1.91723\n[700]\tvalidation_0-mlogloss:1.88487\tvalidation_1-mlogloss:1.91562\n[800]\tvalidation_0-mlogloss:1.87940\tvalidation_1-mlogloss:1.91415\n[900]\tvalidation_0-mlogloss:1.87412\tvalidation_1-mlogloss:1.91284\n[1000]\tvalidation_0-mlogloss:1.86912\tvalidation_1-mlogloss:1.91168\n[1100]\tvalidation_0-mlogloss:1.86427\tvalidation_1-mlogloss:1.91063\n[1200]\tvalidation_0-mlogloss:1.85964\tvalidation_1-mlogloss:1.90977\n[1300]\tvalidation_0-mlogloss:1.85509\tvalidation_1-mlogloss:1.90891\n[1400]\tvalidation_0-mlogloss:1.85073\tvalidation_1-mlogloss:1.90818\n[1500]\tvalidation_0-mlogloss:1.84639\tvalidation_1-mlogloss:1.90756\n[1600]\tvalidation_0-mlogloss:1.84217\tvalidation_1-mlogloss:1.90698\n[1700]\tvalidation_0-mlogloss:1.83806\tvalidation_1-mlogloss:1.90646\n[1800]\tvalidation_0-mlogloss:1.83415\tvalidation_1-mlogloss:1.90602\n[1900]\tvalidation_0-mlogloss:1.83021\tvalidation_1-mlogloss:1.90558\n[2000]\tvalidation_0-mlogloss:1.82627\tvalidation_1-mlogloss:1.90519\n[2100]\tvalidation_0-mlogloss:1.82247\tvalidation_1-mlogloss:1.90484\n[2200]\tvalidation_0-mlogloss:1.81885\tvalidation_1-mlogloss:1.90454\n[2300]\tvalidation_0-mlogloss:1.81519\tvalidation_1-mlogloss:1.90428\n[2400]\tvalidation_0-mlogloss:1.81165\tvalidation_1-mlogloss:1.90405\n[2500]\tvalidation_0-mlogloss:1.80811\tvalidation_1-mlogloss:1.90386\n[2600]\tvalidation_0-mlogloss:1.80467\tvalidation_1-mlogloss:1.90372\n[2700]\tvalidation_0-mlogloss:1.80130\tvalidation_1-mlogloss:1.90358\n[2800]\tvalidation_0-mlogloss:1.79790\tvalidation_1-mlogloss:1.90348\n[2900]\tvalidation_0-mlogloss:1.79461\tvalidation_1-mlogloss:1.90335\n[3000]\tvalidation_0-mlogloss:1.79125\tvalidation_1-mlogloss:1.90328\n[3100]\tvalidation_0-mlogloss:1.78806\tvalidation_1-mlogloss:1.90317\n[3200]\tvalidation_0-mlogloss:1.78482\tvalidation_1-mlogloss:1.90313\n[3300]\tvalidation_0-mlogloss:1.78162\tvalidation_1-mlogloss:1.90309\n[3400]\tvalidation_0-mlogloss:1.77861\tvalidation_1-mlogloss:1.90308\n[3490]\tvalidation_0-mlogloss:1.77586\tvalidation_1-mlogloss:1.90311\n    XGBoost Fold 4 Final Train Loss: 1.7759\n    XGBoost Fold 4 Final Valid Loss: 1.9031\n  Fold 5/10\n[0]\tvalidation_0-mlogloss:1.94563\tvalidation_1-mlogloss:1.94567\n[100]\tvalidation_0-mlogloss:1.92849\tvalidation_1-mlogloss:1.93350\n[200]\tvalidation_0-mlogloss:1.91828\tvalidation_1-mlogloss:1.92799\n[300]\tvalidation_0-mlogloss:1.91011\tvalidation_1-mlogloss:1.92413\n[400]\tvalidation_0-mlogloss:1.90299\tvalidation_1-mlogloss:1.92125\n[500]\tvalidation_0-mlogloss:1.89668\tvalidation_1-mlogloss:1.91897\n[600]\tvalidation_0-mlogloss:1.89071\tvalidation_1-mlogloss:1.91693\n[700]\tvalidation_0-mlogloss:1.88488\tvalidation_1-mlogloss:1.91523\n[800]\tvalidation_0-mlogloss:1.87934\tvalidation_1-mlogloss:1.91372\n[900]\tvalidation_0-mlogloss:1.87408\tvalidation_1-mlogloss:1.91240\n[1000]\tvalidation_0-mlogloss:1.86909\tvalidation_1-mlogloss:1.91124\n[1100]\tvalidation_0-mlogloss:1.86421\tvalidation_1-mlogloss:1.91014\n[1200]\tvalidation_0-mlogloss:1.85957\tvalidation_1-mlogloss:1.90930\n[1300]\tvalidation_0-mlogloss:1.85500\tvalidation_1-mlogloss:1.90848\n[1400]\tvalidation_0-mlogloss:1.85065\tvalidation_1-mlogloss:1.90777\n[1500]\tvalidation_0-mlogloss:1.84631\tvalidation_1-mlogloss:1.90703\n[1600]\tvalidation_0-mlogloss:1.84206\tvalidation_1-mlogloss:1.90644\n[1700]\tvalidation_0-mlogloss:1.83795\tvalidation_1-mlogloss:1.90587\n[1800]\tvalidation_0-mlogloss:1.83406\tvalidation_1-mlogloss:1.90534\n[1900]\tvalidation_0-mlogloss:1.83017\tvalidation_1-mlogloss:1.90490\n[2000]\tvalidation_0-mlogloss:1.82630\tvalidation_1-mlogloss:1.90454\n[2100]\tvalidation_0-mlogloss:1.82250\tvalidation_1-mlogloss:1.90411\n[2200]\tvalidation_0-mlogloss:1.81890\tvalidation_1-mlogloss:1.90374\n[2300]\tvalidation_0-mlogloss:1.81522\tvalidation_1-mlogloss:1.90344\n[2400]\tvalidation_0-mlogloss:1.81168\tvalidation_1-mlogloss:1.90321\n[2500]\tvalidation_0-mlogloss:1.80818\tvalidation_1-mlogloss:1.90297\n[2600]\tvalidation_0-mlogloss:1.80473\tvalidation_1-mlogloss:1.90273\n[2700]\tvalidation_0-mlogloss:1.80138\tvalidation_1-mlogloss:1.90258\n[2800]\tvalidation_0-mlogloss:1.79798\tvalidation_1-mlogloss:1.90243\n[2900]\tvalidation_0-mlogloss:1.79467\tvalidation_1-mlogloss:1.90233\n[3000]\tvalidation_0-mlogloss:1.79131\tvalidation_1-mlogloss:1.90226\n[3100]\tvalidation_0-mlogloss:1.78809\tvalidation_1-mlogloss:1.90219\n[3200]\tvalidation_0-mlogloss:1.78488\tvalidation_1-mlogloss:1.90216\n[3300]\tvalidation_0-mlogloss:1.78166\tvalidation_1-mlogloss:1.90211\n[3400]\tvalidation_0-mlogloss:1.77868\tvalidation_1-mlogloss:1.90207\n[3500]\tvalidation_0-mlogloss:1.77564\tvalidation_1-mlogloss:1.90209\n[3517]\tvalidation_0-mlogloss:1.77514\tvalidation_1-mlogloss:1.90210\n    XGBoost Fold 5 Final Train Loss: 1.7751\n    XGBoost Fold 5 Final Valid Loss: 1.9021\n  Fold 6/10\n[0]\tvalidation_0-mlogloss:1.94564\tvalidation_1-mlogloss:1.94567\n[100]\tvalidation_0-mlogloss:1.92863\tvalidation_1-mlogloss:1.93349\n[200]\tvalidation_0-mlogloss:1.91843\tvalidation_1-mlogloss:1.92801\n[300]\tvalidation_0-mlogloss:1.91022\tvalidation_1-mlogloss:1.92416\n[400]\tvalidation_0-mlogloss:1.90310\tvalidation_1-mlogloss:1.92124\n[500]\tvalidation_0-mlogloss:1.89676\tvalidation_1-mlogloss:1.91889\n[600]\tvalidation_0-mlogloss:1.89072\tvalidation_1-mlogloss:1.91687\n[700]\tvalidation_0-mlogloss:1.88485\tvalidation_1-mlogloss:1.91525\n[800]\tvalidation_0-mlogloss:1.87933\tvalidation_1-mlogloss:1.91381\n[900]\tvalidation_0-mlogloss:1.87407\tvalidation_1-mlogloss:1.91248\n[1000]\tvalidation_0-mlogloss:1.86908\tvalidation_1-mlogloss:1.91131\n[1100]\tvalidation_0-mlogloss:1.86422\tvalidation_1-mlogloss:1.91026\n[1200]\tvalidation_0-mlogloss:1.85957\tvalidation_1-mlogloss:1.90936\n[1300]\tvalidation_0-mlogloss:1.85501\tvalidation_1-mlogloss:1.90853\n[1400]\tvalidation_0-mlogloss:1.85064\tvalidation_1-mlogloss:1.90774\n[1500]\tvalidation_0-mlogloss:1.84636\tvalidation_1-mlogloss:1.90706\n[1600]\tvalidation_0-mlogloss:1.84211\tvalidation_1-mlogloss:1.90648\n[1700]\tvalidation_0-mlogloss:1.83802\tvalidation_1-mlogloss:1.90601\n[1800]\tvalidation_0-mlogloss:1.83406\tvalidation_1-mlogloss:1.90553\n[1900]\tvalidation_0-mlogloss:1.83019\tvalidation_1-mlogloss:1.90515\n[2000]\tvalidation_0-mlogloss:1.82626\tvalidation_1-mlogloss:1.90472\n[2100]\tvalidation_0-mlogloss:1.82246\tvalidation_1-mlogloss:1.90429\n[2200]\tvalidation_0-mlogloss:1.81890\tvalidation_1-mlogloss:1.90402\n[2300]\tvalidation_0-mlogloss:1.81522\tvalidation_1-mlogloss:1.90370\n[2400]\tvalidation_0-mlogloss:1.81167\tvalidation_1-mlogloss:1.90342\n[2500]\tvalidation_0-mlogloss:1.80816\tvalidation_1-mlogloss:1.90319\n[2600]\tvalidation_0-mlogloss:1.80470\tvalidation_1-mlogloss:1.90307\n[2700]\tvalidation_0-mlogloss:1.80137\tvalidation_1-mlogloss:1.90283\n[2800]\tvalidation_0-mlogloss:1.79797\tvalidation_1-mlogloss:1.90270\n[2900]\tvalidation_0-mlogloss:1.79466\tvalidation_1-mlogloss:1.90251\n[3000]\tvalidation_0-mlogloss:1.79131\tvalidation_1-mlogloss:1.90236\n[3100]\tvalidation_0-mlogloss:1.78811\tvalidation_1-mlogloss:1.90223\n[3200]\tvalidation_0-mlogloss:1.78487\tvalidation_1-mlogloss:1.90212\n[3300]\tvalidation_0-mlogloss:1.78165\tvalidation_1-mlogloss:1.90205\n[3400]\tvalidation_0-mlogloss:1.77864\tvalidation_1-mlogloss:1.90197\n[3500]\tvalidation_0-mlogloss:1.77559\tvalidation_1-mlogloss:1.90194\n[3573]\tvalidation_0-mlogloss:1.77346\tvalidation_1-mlogloss:1.90196\n    XGBoost Fold 6 Final Train Loss: 1.7735\n    XGBoost Fold 6 Final Valid Loss: 1.9020\n  Fold 7/10\n[0]\tvalidation_0-mlogloss:1.94564\tvalidation_1-mlogloss:1.94567\n[100]\tvalidation_0-mlogloss:1.92859\tvalidation_1-mlogloss:1.93353\n[200]\tvalidation_0-mlogloss:1.91827\tvalidation_1-mlogloss:1.92802\n[300]\tvalidation_0-mlogloss:1.91003\tvalidation_1-mlogloss:1.92429\n[400]\tvalidation_0-mlogloss:1.90293\tvalidation_1-mlogloss:1.92154\n[500]\tvalidation_0-mlogloss:1.89659\tvalidation_1-mlogloss:1.91935\n[600]\tvalidation_0-mlogloss:1.89054\tvalidation_1-mlogloss:1.91749\n[700]\tvalidation_0-mlogloss:1.88471\tvalidation_1-mlogloss:1.91598\n[800]\tvalidation_0-mlogloss:1.87924\tvalidation_1-mlogloss:1.91465\n[900]\tvalidation_0-mlogloss:1.87398\tvalidation_1-mlogloss:1.91344\n[1000]\tvalidation_0-mlogloss:1.86898\tvalidation_1-mlogloss:1.91235\n[1100]\tvalidation_0-mlogloss:1.86411\tvalidation_1-mlogloss:1.91136\n[1200]\tvalidation_0-mlogloss:1.85947\tvalidation_1-mlogloss:1.91052\n[1300]\tvalidation_0-mlogloss:1.85489\tvalidation_1-mlogloss:1.90983\n[1400]\tvalidation_0-mlogloss:1.85056\tvalidation_1-mlogloss:1.90913\n[1500]\tvalidation_0-mlogloss:1.84625\tvalidation_1-mlogloss:1.90849\n[1600]\tvalidation_0-mlogloss:1.84202\tvalidation_1-mlogloss:1.90792\n[1700]\tvalidation_0-mlogloss:1.83792\tvalidation_1-mlogloss:1.90741\n[1800]\tvalidation_0-mlogloss:1.83398\tvalidation_1-mlogloss:1.90699\n[1900]\tvalidation_0-mlogloss:1.83005\tvalidation_1-mlogloss:1.90657\n[2000]\tvalidation_0-mlogloss:1.82613\tvalidation_1-mlogloss:1.90622\n[2100]\tvalidation_0-mlogloss:1.82230\tvalidation_1-mlogloss:1.90592\n[2200]\tvalidation_0-mlogloss:1.81870\tvalidation_1-mlogloss:1.90564\n[2300]\tvalidation_0-mlogloss:1.81501\tvalidation_1-mlogloss:1.90542\n[2400]\tvalidation_0-mlogloss:1.81148\tvalidation_1-mlogloss:1.90523\n[2500]\tvalidation_0-mlogloss:1.80797\tvalidation_1-mlogloss:1.90509\n[2600]\tvalidation_0-mlogloss:1.80450\tvalidation_1-mlogloss:1.90498\n[2700]\tvalidation_0-mlogloss:1.80116\tvalidation_1-mlogloss:1.90489\n[2800]\tvalidation_0-mlogloss:1.79777\tvalidation_1-mlogloss:1.90476\n[2900]\tvalidation_0-mlogloss:1.79445\tvalidation_1-mlogloss:1.90465\n[3000]\tvalidation_0-mlogloss:1.79107\tvalidation_1-mlogloss:1.90455\n[3100]\tvalidation_0-mlogloss:1.78787\tvalidation_1-mlogloss:1.90445\n[3200]\tvalidation_0-mlogloss:1.78462\tvalidation_1-mlogloss:1.90433\n[3300]\tvalidation_0-mlogloss:1.78141\tvalidation_1-mlogloss:1.90432\n[3400]\tvalidation_0-mlogloss:1.77841\tvalidation_1-mlogloss:1.90433\n[3434]\tvalidation_0-mlogloss:1.77741\tvalidation_1-mlogloss:1.90433\n    XGBoost Fold 7 Final Train Loss: 1.7774\n    XGBoost Fold 7 Final Valid Loss: 1.9043\n  Fold 8/10\n[0]\tvalidation_0-mlogloss:1.94564\tvalidation_1-mlogloss:1.94568\n[100]\tvalidation_0-mlogloss:1.92848\tvalidation_1-mlogloss:1.93387\n[200]\tvalidation_0-mlogloss:1.91825\tvalidation_1-mlogloss:1.92859\n[300]\tvalidation_0-mlogloss:1.90997\tvalidation_1-mlogloss:1.92495\n[400]\tvalidation_0-mlogloss:1.90292\tvalidation_1-mlogloss:1.92221\n[500]\tvalidation_0-mlogloss:1.89647\tvalidation_1-mlogloss:1.91997\n[600]\tvalidation_0-mlogloss:1.89041\tvalidation_1-mlogloss:1.91808\n[700]\tvalidation_0-mlogloss:1.88457\tvalidation_1-mlogloss:1.91647\n[800]\tvalidation_0-mlogloss:1.87905\tvalidation_1-mlogloss:1.91506\n[900]\tvalidation_0-mlogloss:1.87376\tvalidation_1-mlogloss:1.91389\n[1000]\tvalidation_0-mlogloss:1.86878\tvalidation_1-mlogloss:1.91279\n[1100]\tvalidation_0-mlogloss:1.86390\tvalidation_1-mlogloss:1.91178\n[1200]\tvalidation_0-mlogloss:1.85932\tvalidation_1-mlogloss:1.91103\n[1300]\tvalidation_0-mlogloss:1.85476\tvalidation_1-mlogloss:1.91024\n[1400]\tvalidation_0-mlogloss:1.85042\tvalidation_1-mlogloss:1.90958\n[1500]\tvalidation_0-mlogloss:1.84614\tvalidation_1-mlogloss:1.90896\n[1600]\tvalidation_0-mlogloss:1.84186\tvalidation_1-mlogloss:1.90837\n[1700]\tvalidation_0-mlogloss:1.83775\tvalidation_1-mlogloss:1.90786\n[1800]\tvalidation_0-mlogloss:1.83381\tvalidation_1-mlogloss:1.90739\n[1900]\tvalidation_0-mlogloss:1.82988\tvalidation_1-mlogloss:1.90704\n[2000]\tvalidation_0-mlogloss:1.82594\tvalidation_1-mlogloss:1.90665\n[2100]\tvalidation_0-mlogloss:1.82213\tvalidation_1-mlogloss:1.90638\n[2200]\tvalidation_0-mlogloss:1.81852\tvalidation_1-mlogloss:1.90611\n[2300]\tvalidation_0-mlogloss:1.81487\tvalidation_1-mlogloss:1.90587\n[2400]\tvalidation_0-mlogloss:1.81132\tvalidation_1-mlogloss:1.90562\n[2500]\tvalidation_0-mlogloss:1.80783\tvalidation_1-mlogloss:1.90544\n[2600]\tvalidation_0-mlogloss:1.80432\tvalidation_1-mlogloss:1.90526\n[2700]\tvalidation_0-mlogloss:1.80099\tvalidation_1-mlogloss:1.90509\n[2800]\tvalidation_0-mlogloss:1.79759\tvalidation_1-mlogloss:1.90501\n[2900]\tvalidation_0-mlogloss:1.79433\tvalidation_1-mlogloss:1.90496\n[3000]\tvalidation_0-mlogloss:1.79102\tvalidation_1-mlogloss:1.90486\n[3100]\tvalidation_0-mlogloss:1.78783\tvalidation_1-mlogloss:1.90482\n[3200]\tvalidation_0-mlogloss:1.78460\tvalidation_1-mlogloss:1.90482\n[3282]\tvalidation_0-mlogloss:1.78199\tvalidation_1-mlogloss:1.90479\n    XGBoost Fold 8 Final Train Loss: 1.7820\n    XGBoost Fold 8 Final Valid Loss: 1.9048\n  Fold 9/10\n[0]\tvalidation_0-mlogloss:1.94564\tvalidation_1-mlogloss:1.94567\n[100]\tvalidation_0-mlogloss:1.92861\tvalidation_1-mlogloss:1.93365\n[200]\tvalidation_0-mlogloss:1.91841\tvalidation_1-mlogloss:1.92807\n[300]\tvalidation_0-mlogloss:1.91021\tvalidation_1-mlogloss:1.92422\n[400]\tvalidation_0-mlogloss:1.90317\tvalidation_1-mlogloss:1.92133\n[500]\tvalidation_0-mlogloss:1.89676\tvalidation_1-mlogloss:1.91896\n[600]\tvalidation_0-mlogloss:1.89080\tvalidation_1-mlogloss:1.91698\n[700]\tvalidation_0-mlogloss:1.88502\tvalidation_1-mlogloss:1.91523\n[800]\tvalidation_0-mlogloss:1.87949\tvalidation_1-mlogloss:1.91373\n[900]\tvalidation_0-mlogloss:1.87420\tvalidation_1-mlogloss:1.91238\n[1000]\tvalidation_0-mlogloss:1.86913\tvalidation_1-mlogloss:1.91122\n[1100]\tvalidation_0-mlogloss:1.86422\tvalidation_1-mlogloss:1.91016\n[1200]\tvalidation_0-mlogloss:1.85959\tvalidation_1-mlogloss:1.90923\n[1300]\tvalidation_0-mlogloss:1.85503\tvalidation_1-mlogloss:1.90839\n[1400]\tvalidation_0-mlogloss:1.85073\tvalidation_1-mlogloss:1.90764\n[1500]\tvalidation_0-mlogloss:1.84641\tvalidation_1-mlogloss:1.90704\n[1600]\tvalidation_0-mlogloss:1.84215\tvalidation_1-mlogloss:1.90643\n[1700]\tvalidation_0-mlogloss:1.83803\tvalidation_1-mlogloss:1.90587\n[1800]\tvalidation_0-mlogloss:1.83412\tvalidation_1-mlogloss:1.90539\n[1900]\tvalidation_0-mlogloss:1.83023\tvalidation_1-mlogloss:1.90497\n[2000]\tvalidation_0-mlogloss:1.82634\tvalidation_1-mlogloss:1.90458\n[2100]\tvalidation_0-mlogloss:1.82251\tvalidation_1-mlogloss:1.90419\n[2200]\tvalidation_0-mlogloss:1.81891\tvalidation_1-mlogloss:1.90389\n[2300]\tvalidation_0-mlogloss:1.81528\tvalidation_1-mlogloss:1.90357\n[2400]\tvalidation_0-mlogloss:1.81173\tvalidation_1-mlogloss:1.90332\n[2500]\tvalidation_0-mlogloss:1.80824\tvalidation_1-mlogloss:1.90312\n[2600]\tvalidation_0-mlogloss:1.80477\tvalidation_1-mlogloss:1.90295\n[2700]\tvalidation_0-mlogloss:1.80139\tvalidation_1-mlogloss:1.90280\n[2800]\tvalidation_0-mlogloss:1.79800\tvalidation_1-mlogloss:1.90265\n[2900]\tvalidation_0-mlogloss:1.79468\tvalidation_1-mlogloss:1.90253\n[3000]\tvalidation_0-mlogloss:1.79130\tvalidation_1-mlogloss:1.90240\n[3100]\tvalidation_0-mlogloss:1.78810\tvalidation_1-mlogloss:1.90234\n[3200]\tvalidation_0-mlogloss:1.78488\tvalidation_1-mlogloss:1.90226\n[3300]\tvalidation_0-mlogloss:1.78169\tvalidation_1-mlogloss:1.90220\n[3400]\tvalidation_0-mlogloss:1.77868\tvalidation_1-mlogloss:1.90216\n[3500]\tvalidation_0-mlogloss:1.77565\tvalidation_1-mlogloss:1.90216\n[3530]\tvalidation_0-mlogloss:1.77477\tvalidation_1-mlogloss:1.90216\n    XGBoost Fold 9 Final Train Loss: 1.7747\n    XGBoost Fold 9 Final Valid Loss: 1.9022\n  Fold 10/10\n[0]\tvalidation_0-mlogloss:1.94564\tvalidation_1-mlogloss:1.94567\n[100]\tvalidation_0-mlogloss:1.92853\tvalidation_1-mlogloss:1.93375\n[200]\tvalidation_0-mlogloss:1.91827\tvalidation_1-mlogloss:1.92838\n[300]\tvalidation_0-mlogloss:1.91009\tvalidation_1-mlogloss:1.92469\n[400]\tvalidation_0-mlogloss:1.90301\tvalidation_1-mlogloss:1.92190\n[500]\tvalidation_0-mlogloss:1.89661\tvalidation_1-mlogloss:1.91966\n[600]\tvalidation_0-mlogloss:1.89063\tvalidation_1-mlogloss:1.91779\n[700]\tvalidation_0-mlogloss:1.88481\tvalidation_1-mlogloss:1.91620\n[800]\tvalidation_0-mlogloss:1.87932\tvalidation_1-mlogloss:1.91482\n[900]\tvalidation_0-mlogloss:1.87399\tvalidation_1-mlogloss:1.91357\n[1000]\tvalidation_0-mlogloss:1.86901\tvalidation_1-mlogloss:1.91250\n[1100]\tvalidation_0-mlogloss:1.86416\tvalidation_1-mlogloss:1.91152\n[1200]\tvalidation_0-mlogloss:1.85952\tvalidation_1-mlogloss:1.91072\n[1300]\tvalidation_0-mlogloss:1.85494\tvalidation_1-mlogloss:1.90993\n[1400]\tvalidation_0-mlogloss:1.85062\tvalidation_1-mlogloss:1.90930\n[1500]\tvalidation_0-mlogloss:1.84629\tvalidation_1-mlogloss:1.90868\n[1600]\tvalidation_0-mlogloss:1.84201\tvalidation_1-mlogloss:1.90812\n[1700]\tvalidation_0-mlogloss:1.83787\tvalidation_1-mlogloss:1.90758\n[1800]\tvalidation_0-mlogloss:1.83390\tvalidation_1-mlogloss:1.90713\n[1900]\tvalidation_0-mlogloss:1.83000\tvalidation_1-mlogloss:1.90671\n[2000]\tvalidation_0-mlogloss:1.82610\tvalidation_1-mlogloss:1.90635\n[2100]\tvalidation_0-mlogloss:1.82227\tvalidation_1-mlogloss:1.90600\n[2200]\tvalidation_0-mlogloss:1.81867\tvalidation_1-mlogloss:1.90578\n[2300]\tvalidation_0-mlogloss:1.81497\tvalidation_1-mlogloss:1.90554\n[2400]\tvalidation_0-mlogloss:1.81144\tvalidation_1-mlogloss:1.90534\n[2500]\tvalidation_0-mlogloss:1.80797\tvalidation_1-mlogloss:1.90517\n[2600]\tvalidation_0-mlogloss:1.80450\tvalidation_1-mlogloss:1.90503\n[2700]\tvalidation_0-mlogloss:1.80113\tvalidation_1-mlogloss:1.90488\n[2800]\tvalidation_0-mlogloss:1.79773\tvalidation_1-mlogloss:1.90478\n[2900]\tvalidation_0-mlogloss:1.79442\tvalidation_1-mlogloss:1.90471\n[3000]\tvalidation_0-mlogloss:1.79108\tvalidation_1-mlogloss:1.90462\n[3100]\tvalidation_0-mlogloss:1.78788\tvalidation_1-mlogloss:1.90457\n[3200]\tvalidation_0-mlogloss:1.78464\tvalidation_1-mlogloss:1.90457\n[3300]\tvalidation_0-mlogloss:1.78141\tvalidation_1-mlogloss:1.90458\n[3360]\tvalidation_0-mlogloss:1.77955\tvalidation_1-mlogloss:1.90456\n    XGBoost Fold 10 Final Train Loss: 1.7796\n    XGBoost Fold 10 Final Valid Loss: 1.9046\n\n Training Base Model: LGB\n  Fold 1/10\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/sklearn/preprocessing/_label.py:99: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n  y = column_or_1d(y, warn=True)\n/usr/local/lib/python3.11/dist-packages/sklearn/preprocessing/_label.py:134: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n  y = column_or_1d(y, dtype=self.classes_.dtype, warn=True)\n/usr/local/lib/python3.11/dist-packages/sklearn/preprocessing/_label.py:134: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n  y = column_or_1d(y, dtype=self.classes_.dtype, warn=True)\n","output_type":"stream"},{"name":"stdout","text":"[100]\ttraining's multi_logloss: 1.92734\tvalid_1's multi_logloss: 1.93177\n[200]\ttraining's multi_logloss: 1.91892\tvalid_1's multi_logloss: 1.92751\n[300]\ttraining's multi_logloss: 1.91244\tvalid_1's multi_logloss: 1.92529\n[400]\ttraining's multi_logloss: 1.90672\tvalid_1's multi_logloss: 1.92384\n[500]\ttraining's multi_logloss: 1.90135\tvalid_1's multi_logloss: 1.92267\n[600]\ttraining's multi_logloss: 1.89626\tvalid_1's multi_logloss: 1.92159\n[700]\ttraining's multi_logloss: 1.89147\tvalid_1's multi_logloss: 1.92084\n[800]\ttraining's multi_logloss: 1.88694\tvalid_1's multi_logloss: 1.92026\n[900]\ttraining's multi_logloss: 1.88252\tvalid_1's multi_logloss: 1.91966\n[1000]\ttraining's multi_logloss: 1.87828\tvalid_1's multi_logloss: 1.91923\n[1100]\ttraining's multi_logloss: 1.8742\tvalid_1's multi_logloss: 1.91893\n[1200]\ttraining's multi_logloss: 1.87018\tvalid_1's multi_logloss: 1.91857\n[1300]\ttraining's multi_logloss: 1.86622\tvalid_1's multi_logloss: 1.91834\n[1400]\ttraining's multi_logloss: 1.86239\tvalid_1's multi_logloss: 1.91813\n[1500]\ttraining's multi_logloss: 1.8586\tvalid_1's multi_logloss: 1.91795\n[1600]\ttraining's multi_logloss: 1.85483\tvalid_1's multi_logloss: 1.9178\n[1700]\ttraining's multi_logloss: 1.8512\tvalid_1's multi_logloss: 1.91767\n[1800]\ttraining's multi_logloss: 1.84761\tvalid_1's multi_logloss: 1.91755\n[1900]\ttraining's multi_logloss: 1.84409\tvalid_1's multi_logloss: 1.9175\n[2000]\ttraining's multi_logloss: 1.84059\tvalid_1's multi_logloss: 1.91742\n[2100]\ttraining's multi_logloss: 1.83713\tvalid_1's multi_logloss: 1.91732\n[2200]\ttraining's multi_logloss: 1.83377\tvalid_1's multi_logloss: 1.91726\n[2300]\ttraining's multi_logloss: 1.83037\tvalid_1's multi_logloss: 1.91722\n[2400]\ttraining's multi_logloss: 1.82698\tvalid_1's multi_logloss: 1.91716\n[2500]\ttraining's multi_logloss: 1.82366\tvalid_1's multi_logloss: 1.91712\n[2600]\ttraining's multi_logloss: 1.82038\tvalid_1's multi_logloss: 1.91708\n[2700]\ttraining's multi_logloss: 1.8171\tvalid_1's multi_logloss: 1.91706\n[2800]\ttraining's multi_logloss: 1.81388\tvalid_1's multi_logloss: 1.91703\n[2900]\ttraining's multi_logloss: 1.81065\tvalid_1's multi_logloss: 1.91698\n[3000]\ttraining's multi_logloss: 1.8074\tvalid_1's multi_logloss: 1.91687\n[3100]\ttraining's multi_logloss: 1.80422\tvalid_1's multi_logloss: 1.91687\n    LightGBM Fold 1 Best Iteration: 3042\n    LightGBM Fold 1 Final Valid Loss (Best Score): 1.9169\n  Fold 2/10\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/sklearn/preprocessing/_label.py:99: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n  y = column_or_1d(y, warn=True)\n/usr/local/lib/python3.11/dist-packages/sklearn/preprocessing/_label.py:134: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n  y = column_or_1d(y, dtype=self.classes_.dtype, warn=True)\n/usr/local/lib/python3.11/dist-packages/sklearn/preprocessing/_label.py:134: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n  y = column_or_1d(y, dtype=self.classes_.dtype, warn=True)\n","output_type":"stream"},{"name":"stdout","text":"[100]\ttraining's multi_logloss: 1.92743\tvalid_1's multi_logloss: 1.93171\n[200]\ttraining's multi_logloss: 1.9189\tvalid_1's multi_logloss: 1.92732\n[300]\ttraining's multi_logloss: 1.91249\tvalid_1's multi_logloss: 1.92524\n[400]\ttraining's multi_logloss: 1.90672\tvalid_1's multi_logloss: 1.92368\n[500]\ttraining's multi_logloss: 1.90135\tvalid_1's multi_logloss: 1.9224\n[600]\ttraining's multi_logloss: 1.89631\tvalid_1's multi_logloss: 1.92134\n[700]\ttraining's multi_logloss: 1.89149\tvalid_1's multi_logloss: 1.92052\n[800]\ttraining's multi_logloss: 1.88695\tvalid_1's multi_logloss: 1.9198\n[900]\ttraining's multi_logloss: 1.88252\tvalid_1's multi_logloss: 1.91927\n[1000]\ttraining's multi_logloss: 1.87827\tvalid_1's multi_logloss: 1.91881\n[1100]\ttraining's multi_logloss: 1.87418\tvalid_1's multi_logloss: 1.9185\n[1200]\ttraining's multi_logloss: 1.87017\tvalid_1's multi_logloss: 1.91825\n[1300]\ttraining's multi_logloss: 1.86622\tvalid_1's multi_logloss: 1.91801\n[1400]\ttraining's multi_logloss: 1.86233\tvalid_1's multi_logloss: 1.91783\n[1500]\ttraining's multi_logloss: 1.85857\tvalid_1's multi_logloss: 1.91768\n[1600]\ttraining's multi_logloss: 1.85486\tvalid_1's multi_logloss: 1.91757\n[1700]\ttraining's multi_logloss: 1.8512\tvalid_1's multi_logloss: 1.91741\n[1800]\ttraining's multi_logloss: 1.84762\tvalid_1's multi_logloss: 1.91733\n[1900]\ttraining's multi_logloss: 1.84405\tvalid_1's multi_logloss: 1.91721\n[2000]\ttraining's multi_logloss: 1.84057\tvalid_1's multi_logloss: 1.9171\n[2100]\ttraining's multi_logloss: 1.83715\tvalid_1's multi_logloss: 1.91702\n[2200]\ttraining's multi_logloss: 1.83371\tvalid_1's multi_logloss: 1.91699\n[2300]\ttraining's multi_logloss: 1.83033\tvalid_1's multi_logloss: 1.91693\n[2400]\ttraining's multi_logloss: 1.82701\tvalid_1's multi_logloss: 1.91692\n[2500]\ttraining's multi_logloss: 1.82373\tvalid_1's multi_logloss: 1.91682\n[2600]\ttraining's multi_logloss: 1.82046\tvalid_1's multi_logloss: 1.91676\n[2700]\ttraining's multi_logloss: 1.81719\tvalid_1's multi_logloss: 1.91669\n[2800]\ttraining's multi_logloss: 1.81396\tvalid_1's multi_logloss: 1.91664\n[2900]\ttraining's multi_logloss: 1.81074\tvalid_1's multi_logloss: 1.91664\n    LightGBM Fold 2 Best Iteration: 2806\n    LightGBM Fold 2 Final Valid Loss (Best Score): 1.9166\n  Fold 3/10\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/sklearn/preprocessing/_label.py:99: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n  y = column_or_1d(y, warn=True)\n/usr/local/lib/python3.11/dist-packages/sklearn/preprocessing/_label.py:134: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n  y = column_or_1d(y, dtype=self.classes_.dtype, warn=True)\n/usr/local/lib/python3.11/dist-packages/sklearn/preprocessing/_label.py:134: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n  y = column_or_1d(y, dtype=self.classes_.dtype, warn=True)\n","output_type":"stream"},{"name":"stdout","text":"[100]\ttraining's multi_logloss: 1.92739\tvalid_1's multi_logloss: 1.93169\n[200]\ttraining's multi_logloss: 1.91886\tvalid_1's multi_logloss: 1.92747\n[300]\ttraining's multi_logloss: 1.91223\tvalid_1's multi_logloss: 1.92522\n[400]\ttraining's multi_logloss: 1.90645\tvalid_1's multi_logloss: 1.92365\n[500]\ttraining's multi_logloss: 1.9011\tvalid_1's multi_logloss: 1.92259\n[600]\ttraining's multi_logloss: 1.89605\tvalid_1's multi_logloss: 1.9217\n[700]\ttraining's multi_logloss: 1.89135\tvalid_1's multi_logloss: 1.92101\n[800]\ttraining's multi_logloss: 1.88677\tvalid_1's multi_logloss: 1.9204\n[900]\ttraining's multi_logloss: 1.88235\tvalid_1's multi_logloss: 1.91991\n[1000]\ttraining's multi_logloss: 1.87809\tvalid_1's multi_logloss: 1.91959\n[1100]\ttraining's multi_logloss: 1.87397\tvalid_1's multi_logloss: 1.9192\n[1200]\ttraining's multi_logloss: 1.86995\tvalid_1's multi_logloss: 1.91896\n[1300]\ttraining's multi_logloss: 1.866\tvalid_1's multi_logloss: 1.9187\n[1400]\ttraining's multi_logloss: 1.86213\tvalid_1's multi_logloss: 1.91854\n[1500]\ttraining's multi_logloss: 1.85829\tvalid_1's multi_logloss: 1.9183\n[1600]\ttraining's multi_logloss: 1.85459\tvalid_1's multi_logloss: 1.91815\n[1700]\ttraining's multi_logloss: 1.85093\tvalid_1's multi_logloss: 1.91789\n[1800]\ttraining's multi_logloss: 1.84734\tvalid_1's multi_logloss: 1.91769\n[1900]\ttraining's multi_logloss: 1.84382\tvalid_1's multi_logloss: 1.91751\n[2000]\ttraining's multi_logloss: 1.84024\tvalid_1's multi_logloss: 1.91745\n[2100]\ttraining's multi_logloss: 1.83677\tvalid_1's multi_logloss: 1.91731\n[2200]\ttraining's multi_logloss: 1.83333\tvalid_1's multi_logloss: 1.91729\n[2300]\ttraining's multi_logloss: 1.82995\tvalid_1's multi_logloss: 1.91725\n[2400]\ttraining's multi_logloss: 1.82663\tvalid_1's multi_logloss: 1.91718\n[2500]\ttraining's multi_logloss: 1.82337\tvalid_1's multi_logloss: 1.91706\n[2600]\ttraining's multi_logloss: 1.8201\tvalid_1's multi_logloss: 1.91701\n[2700]\ttraining's multi_logloss: 1.81682\tvalid_1's multi_logloss: 1.91694\n[2800]\ttraining's multi_logloss: 1.81356\tvalid_1's multi_logloss: 1.91689\n[2900]\ttraining's multi_logloss: 1.81032\tvalid_1's multi_logloss: 1.91689\n    LightGBM Fold 3 Best Iteration: 2837\n    LightGBM Fold 3 Final Valid Loss (Best Score): 1.9169\n  Fold 4/10\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/sklearn/preprocessing/_label.py:99: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n  y = column_or_1d(y, warn=True)\n/usr/local/lib/python3.11/dist-packages/sklearn/preprocessing/_label.py:134: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n  y = column_or_1d(y, dtype=self.classes_.dtype, warn=True)\n/usr/local/lib/python3.11/dist-packages/sklearn/preprocessing/_label.py:134: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n  y = column_or_1d(y, dtype=self.classes_.dtype, warn=True)\n","output_type":"stream"},{"name":"stdout","text":"[100]\ttraining's multi_logloss: 1.92736\tvalid_1's multi_logloss: 1.93121\n[200]\ttraining's multi_logloss: 1.91888\tvalid_1's multi_logloss: 1.92695\n[300]\ttraining's multi_logloss: 1.91243\tvalid_1's multi_logloss: 1.92472\n[400]\ttraining's multi_logloss: 1.90667\tvalid_1's multi_logloss: 1.92313\n[500]\ttraining's multi_logloss: 1.90134\tvalid_1's multi_logloss: 1.92195\n[600]\ttraining's multi_logloss: 1.8963\tvalid_1's multi_logloss: 1.92087\n[700]\ttraining's multi_logloss: 1.89148\tvalid_1's multi_logloss: 1.92008\n[800]\ttraining's multi_logloss: 1.88695\tvalid_1's multi_logloss: 1.9195\n[900]\ttraining's multi_logloss: 1.88253\tvalid_1's multi_logloss: 1.91893\n[1000]\ttraining's multi_logloss: 1.87828\tvalid_1's multi_logloss: 1.91842\n[1100]\ttraining's multi_logloss: 1.87416\tvalid_1's multi_logloss: 1.91804\n[1200]\ttraining's multi_logloss: 1.87009\tvalid_1's multi_logloss: 1.91766\n[1300]\ttraining's multi_logloss: 1.86616\tvalid_1's multi_logloss: 1.9173\n[1400]\ttraining's multi_logloss: 1.86227\tvalid_1's multi_logloss: 1.91704\n[1500]\ttraining's multi_logloss: 1.85845\tvalid_1's multi_logloss: 1.91668\n[1600]\ttraining's multi_logloss: 1.85473\tvalid_1's multi_logloss: 1.91643\n[1700]\ttraining's multi_logloss: 1.85105\tvalid_1's multi_logloss: 1.91626\n[1800]\ttraining's multi_logloss: 1.84743\tvalid_1's multi_logloss: 1.91609\n[1900]\ttraining's multi_logloss: 1.84389\tvalid_1's multi_logloss: 1.91591\n[2000]\ttraining's multi_logloss: 1.84035\tvalid_1's multi_logloss: 1.91578\n[2100]\ttraining's multi_logloss: 1.8369\tvalid_1's multi_logloss: 1.91572\n[2200]\ttraining's multi_logloss: 1.83346\tvalid_1's multi_logloss: 1.91559\n[2300]\ttraining's multi_logloss: 1.83011\tvalid_1's multi_logloss: 1.91553\n[2400]\ttraining's multi_logloss: 1.82675\tvalid_1's multi_logloss: 1.91539\n[2500]\ttraining's multi_logloss: 1.82344\tvalid_1's multi_logloss: 1.91524\n[2600]\ttraining's multi_logloss: 1.82015\tvalid_1's multi_logloss: 1.91513\n[2700]\ttraining's multi_logloss: 1.81695\tvalid_1's multi_logloss: 1.9151\n[2800]\ttraining's multi_logloss: 1.81378\tvalid_1's multi_logloss: 1.91509\n[2900]\ttraining's multi_logloss: 1.81058\tvalid_1's multi_logloss: 1.91507\n[3000]\ttraining's multi_logloss: 1.80741\tvalid_1's multi_logloss: 1.91507\n[3100]\ttraining's multi_logloss: 1.80429\tvalid_1's multi_logloss: 1.91513\n    LightGBM Fold 4 Best Iteration: 3007\n    LightGBM Fold 4 Final Valid Loss (Best Score): 1.9150\n  Fold 5/10\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/sklearn/preprocessing/_label.py:99: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n  y = column_or_1d(y, warn=True)\n/usr/local/lib/python3.11/dist-packages/sklearn/preprocessing/_label.py:134: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n  y = column_or_1d(y, dtype=self.classes_.dtype, warn=True)\n/usr/local/lib/python3.11/dist-packages/sklearn/preprocessing/_label.py:134: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n  y = column_or_1d(y, dtype=self.classes_.dtype, warn=True)\n","output_type":"stream"},{"name":"stdout","text":"[100]\ttraining's multi_logloss: 1.92742\tvalid_1's multi_logloss: 1.93177\n[200]\ttraining's multi_logloss: 1.91888\tvalid_1's multi_logloss: 1.92738\n[300]\ttraining's multi_logloss: 1.91246\tvalid_1's multi_logloss: 1.92515\n[400]\ttraining's multi_logloss: 1.90664\tvalid_1's multi_logloss: 1.92337\n[500]\ttraining's multi_logloss: 1.90129\tvalid_1's multi_logloss: 1.92209\n[600]\ttraining's multi_logloss: 1.89637\tvalid_1's multi_logloss: 1.92112\n[700]\ttraining's multi_logloss: 1.89163\tvalid_1's multi_logloss: 1.92032\n[800]\ttraining's multi_logloss: 1.88707\tvalid_1's multi_logloss: 1.91967\n[900]\ttraining's multi_logloss: 1.88265\tvalid_1's multi_logloss: 1.91909\n[1000]\ttraining's multi_logloss: 1.87835\tvalid_1's multi_logloss: 1.91848\n[1100]\ttraining's multi_logloss: 1.87425\tvalid_1's multi_logloss: 1.9181\n[1200]\ttraining's multi_logloss: 1.87026\tvalid_1's multi_logloss: 1.91778\n[1300]\ttraining's multi_logloss: 1.86637\tvalid_1's multi_logloss: 1.9175\n[1400]\ttraining's multi_logloss: 1.86253\tvalid_1's multi_logloss: 1.91722\n[1500]\ttraining's multi_logloss: 1.85878\tvalid_1's multi_logloss: 1.91693\n[1600]\ttraining's multi_logloss: 1.85503\tvalid_1's multi_logloss: 1.91665\n[1700]\ttraining's multi_logloss: 1.85133\tvalid_1's multi_logloss: 1.91643\n[1800]\ttraining's multi_logloss: 1.84773\tvalid_1's multi_logloss: 1.91629\n[1900]\ttraining's multi_logloss: 1.84416\tvalid_1's multi_logloss: 1.91619\n[2000]\ttraining's multi_logloss: 1.8407\tvalid_1's multi_logloss: 1.91614\n[2100]\ttraining's multi_logloss: 1.83721\tvalid_1's multi_logloss: 1.91603\n[2200]\ttraining's multi_logloss: 1.83373\tvalid_1's multi_logloss: 1.91595\n[2300]\ttraining's multi_logloss: 1.83035\tvalid_1's multi_logloss: 1.9159\n[2400]\ttraining's multi_logloss: 1.82699\tvalid_1's multi_logloss: 1.9158\n[2500]\ttraining's multi_logloss: 1.8237\tvalid_1's multi_logloss: 1.91575\n[2600]\ttraining's multi_logloss: 1.82043\tvalid_1's multi_logloss: 1.91574\n    LightGBM Fold 5 Best Iteration: 2516\n    LightGBM Fold 5 Final Valid Loss (Best Score): 1.9157\n  Fold 6/10\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/sklearn/preprocessing/_label.py:99: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n  y = column_or_1d(y, warn=True)\n/usr/local/lib/python3.11/dist-packages/sklearn/preprocessing/_label.py:134: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n  y = column_or_1d(y, dtype=self.classes_.dtype, warn=True)\n/usr/local/lib/python3.11/dist-packages/sklearn/preprocessing/_label.py:134: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n  y = column_or_1d(y, dtype=self.classes_.dtype, warn=True)\n","output_type":"stream"},{"name":"stdout","text":"[100]\ttraining's multi_logloss: 1.92743\tvalid_1's multi_logloss: 1.93162\n[200]\ttraining's multi_logloss: 1.919\tvalid_1's multi_logloss: 1.92724\n[300]\ttraining's multi_logloss: 1.91252\tvalid_1's multi_logloss: 1.92481\n[400]\ttraining's multi_logloss: 1.90674\tvalid_1's multi_logloss: 1.92312\n[500]\ttraining's multi_logloss: 1.90136\tvalid_1's multi_logloss: 1.92181\n[600]\ttraining's multi_logloss: 1.89635\tvalid_1's multi_logloss: 1.92074\n[700]\ttraining's multi_logloss: 1.89156\tvalid_1's multi_logloss: 1.91995\n[800]\ttraining's multi_logloss: 1.88707\tvalid_1's multi_logloss: 1.91933\n[900]\ttraining's multi_logloss: 1.88262\tvalid_1's multi_logloss: 1.91879\n[1000]\ttraining's multi_logloss: 1.87843\tvalid_1's multi_logloss: 1.91837\n[1100]\ttraining's multi_logloss: 1.87424\tvalid_1's multi_logloss: 1.91803\n[1200]\ttraining's multi_logloss: 1.87025\tvalid_1's multi_logloss: 1.91773\n[1300]\ttraining's multi_logloss: 1.86631\tvalid_1's multi_logloss: 1.91745\n[1400]\ttraining's multi_logloss: 1.86247\tvalid_1's multi_logloss: 1.91719\n[1500]\ttraining's multi_logloss: 1.85874\tvalid_1's multi_logloss: 1.91698\n[1600]\ttraining's multi_logloss: 1.85502\tvalid_1's multi_logloss: 1.91666\n[1700]\ttraining's multi_logloss: 1.85141\tvalid_1's multi_logloss: 1.91644\n[1800]\ttraining's multi_logloss: 1.84779\tvalid_1's multi_logloss: 1.91629\n[1900]\ttraining's multi_logloss: 1.84428\tvalid_1's multi_logloss: 1.91624\n[2000]\ttraining's multi_logloss: 1.84083\tvalid_1's multi_logloss: 1.91621\n[2100]\ttraining's multi_logloss: 1.83735\tvalid_1's multi_logloss: 1.91606\n[2200]\ttraining's multi_logloss: 1.83396\tvalid_1's multi_logloss: 1.91594\n[2300]\ttraining's multi_logloss: 1.83058\tvalid_1's multi_logloss: 1.91584\n[2400]\ttraining's multi_logloss: 1.82722\tvalid_1's multi_logloss: 1.91571\n[2500]\ttraining's multi_logloss: 1.82385\tvalid_1's multi_logloss: 1.91564\n[2600]\ttraining's multi_logloss: 1.82052\tvalid_1's multi_logloss: 1.9156\n[2700]\ttraining's multi_logloss: 1.81726\tvalid_1's multi_logloss: 1.91552\n[2800]\ttraining's multi_logloss: 1.81404\tvalid_1's multi_logloss: 1.91547\n    LightGBM Fold 6 Best Iteration: 2745\n    LightGBM Fold 6 Final Valid Loss (Best Score): 1.9155\n  Fold 7/10\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/sklearn/preprocessing/_label.py:99: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n  y = column_or_1d(y, warn=True)\n/usr/local/lib/python3.11/dist-packages/sklearn/preprocessing/_label.py:134: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n  y = column_or_1d(y, dtype=self.classes_.dtype, warn=True)\n/usr/local/lib/python3.11/dist-packages/sklearn/preprocessing/_label.py:134: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n  y = column_or_1d(y, dtype=self.classes_.dtype, warn=True)\n","output_type":"stream"},{"name":"stdout","text":"[100]\ttraining's multi_logloss: 1.92741\tvalid_1's multi_logloss: 1.93147\n[200]\ttraining's multi_logloss: 1.91894\tvalid_1's multi_logloss: 1.92726\n[300]\ttraining's multi_logloss: 1.91252\tvalid_1's multi_logloss: 1.92494\n[400]\ttraining's multi_logloss: 1.90674\tvalid_1's multi_logloss: 1.92328\n[500]\ttraining's multi_logloss: 1.90148\tvalid_1's multi_logloss: 1.92213\n[600]\ttraining's multi_logloss: 1.89651\tvalid_1's multi_logloss: 1.92116\n[700]\ttraining's multi_logloss: 1.89172\tvalid_1's multi_logloss: 1.92039\n[800]\ttraining's multi_logloss: 1.88717\tvalid_1's multi_logloss: 1.91975\n[900]\ttraining's multi_logloss: 1.88275\tvalid_1's multi_logloss: 1.91915\n[1000]\ttraining's multi_logloss: 1.87845\tvalid_1's multi_logloss: 1.91869\n[1100]\ttraining's multi_logloss: 1.87428\tvalid_1's multi_logloss: 1.91827\n[1200]\ttraining's multi_logloss: 1.87024\tvalid_1's multi_logloss: 1.918\n[1300]\ttraining's multi_logloss: 1.86633\tvalid_1's multi_logloss: 1.91776\n[1400]\ttraining's multi_logloss: 1.86247\tvalid_1's multi_logloss: 1.91759\n[1500]\ttraining's multi_logloss: 1.85871\tvalid_1's multi_logloss: 1.91742\n[1600]\ttraining's multi_logloss: 1.85501\tvalid_1's multi_logloss: 1.91723\n[1700]\ttraining's multi_logloss: 1.85128\tvalid_1's multi_logloss: 1.91708\n[1800]\ttraining's multi_logloss: 1.84761\tvalid_1's multi_logloss: 1.91693\n[1900]\ttraining's multi_logloss: 1.84403\tvalid_1's multi_logloss: 1.91674\n[2000]\ttraining's multi_logloss: 1.8405\tvalid_1's multi_logloss: 1.91667\n[2100]\ttraining's multi_logloss: 1.83701\tvalid_1's multi_logloss: 1.91667\n    LightGBM Fold 7 Best Iteration: 2049\n    LightGBM Fold 7 Final Valid Loss (Best Score): 1.9166\n  Fold 8/10\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/sklearn/preprocessing/_label.py:99: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n  y = column_or_1d(y, warn=True)\n/usr/local/lib/python3.11/dist-packages/sklearn/preprocessing/_label.py:134: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n  y = column_or_1d(y, dtype=self.classes_.dtype, warn=True)\n/usr/local/lib/python3.11/dist-packages/sklearn/preprocessing/_label.py:134: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n  y = column_or_1d(y, dtype=self.classes_.dtype, warn=True)\n","output_type":"stream"},{"name":"stdout","text":"[100]\ttraining's multi_logloss: 1.92733\tvalid_1's multi_logloss: 1.93202\n[200]\ttraining's multi_logloss: 1.91881\tvalid_1's multi_logloss: 1.92775\n[300]\ttraining's multi_logloss: 1.91234\tvalid_1's multi_logloss: 1.92561\n[400]\ttraining's multi_logloss: 1.90661\tvalid_1's multi_logloss: 1.92406\n[500]\ttraining's multi_logloss: 1.90127\tvalid_1's multi_logloss: 1.92273\n[600]\ttraining's multi_logloss: 1.89626\tvalid_1's multi_logloss: 1.92184\n[700]\ttraining's multi_logloss: 1.8915\tvalid_1's multi_logloss: 1.92115\n[800]\ttraining's multi_logloss: 1.88687\tvalid_1's multi_logloss: 1.92041\n[900]\ttraining's multi_logloss: 1.88251\tvalid_1's multi_logloss: 1.91987\n[1000]\ttraining's multi_logloss: 1.8783\tvalid_1's multi_logloss: 1.91943\n[1100]\ttraining's multi_logloss: 1.87425\tvalid_1's multi_logloss: 1.91912\n[1200]\ttraining's multi_logloss: 1.87019\tvalid_1's multi_logloss: 1.91881\n[1300]\ttraining's multi_logloss: 1.8662\tvalid_1's multi_logloss: 1.91847\n[1400]\ttraining's multi_logloss: 1.86229\tvalid_1's multi_logloss: 1.91818\n[1500]\ttraining's multi_logloss: 1.85851\tvalid_1's multi_logloss: 1.91797\n[1600]\ttraining's multi_logloss: 1.85479\tvalid_1's multi_logloss: 1.91779\n[1700]\ttraining's multi_logloss: 1.85117\tvalid_1's multi_logloss: 1.91764\n[1800]\ttraining's multi_logloss: 1.84756\tvalid_1's multi_logloss: 1.91745\n[1900]\ttraining's multi_logloss: 1.84399\tvalid_1's multi_logloss: 1.9173\n[2000]\ttraining's multi_logloss: 1.84046\tvalid_1's multi_logloss: 1.9171\n[2100]\ttraining's multi_logloss: 1.83698\tvalid_1's multi_logloss: 1.917\n[2200]\ttraining's multi_logloss: 1.83355\tvalid_1's multi_logloss: 1.91695\n[2300]\ttraining's multi_logloss: 1.83014\tvalid_1's multi_logloss: 1.9169\n    LightGBM Fold 8 Best Iteration: 2299\n    LightGBM Fold 8 Final Valid Loss (Best Score): 1.9169\n  Fold 9/10\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/sklearn/preprocessing/_label.py:99: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n  y = column_or_1d(y, warn=True)\n/usr/local/lib/python3.11/dist-packages/sklearn/preprocessing/_label.py:134: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n  y = column_or_1d(y, dtype=self.classes_.dtype, warn=True)\n/usr/local/lib/python3.11/dist-packages/sklearn/preprocessing/_label.py:134: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n  y = column_or_1d(y, dtype=self.classes_.dtype, warn=True)\n","output_type":"stream"},{"name":"stdout","text":"[100]\ttraining's multi_logloss: 1.92746\tvalid_1's multi_logloss: 1.93175\n[200]\ttraining's multi_logloss: 1.91902\tvalid_1's multi_logloss: 1.92728\n[300]\ttraining's multi_logloss: 1.91251\tvalid_1's multi_logloss: 1.92484\n[400]\ttraining's multi_logloss: 1.90676\tvalid_1's multi_logloss: 1.92324\n[500]\ttraining's multi_logloss: 1.90144\tvalid_1's multi_logloss: 1.92201\n[600]\ttraining's multi_logloss: 1.89641\tvalid_1's multi_logloss: 1.92092\n[700]\ttraining's multi_logloss: 1.89154\tvalid_1's multi_logloss: 1.91996\n[800]\ttraining's multi_logloss: 1.88694\tvalid_1's multi_logloss: 1.91923\n[900]\ttraining's multi_logloss: 1.88257\tvalid_1's multi_logloss: 1.91867\n[1000]\ttraining's multi_logloss: 1.8783\tvalid_1's multi_logloss: 1.91814\n[1100]\ttraining's multi_logloss: 1.87419\tvalid_1's multi_logloss: 1.91775\n[1200]\ttraining's multi_logloss: 1.87021\tvalid_1's multi_logloss: 1.91737\n[1300]\ttraining's multi_logloss: 1.86626\tvalid_1's multi_logloss: 1.91702\n[1400]\ttraining's multi_logloss: 1.86243\tvalid_1's multi_logloss: 1.91678\n[1500]\ttraining's multi_logloss: 1.85856\tvalid_1's multi_logloss: 1.91658\n[1600]\ttraining's multi_logloss: 1.85476\tvalid_1's multi_logloss: 1.91628\n[1700]\ttraining's multi_logloss: 1.85104\tvalid_1's multi_logloss: 1.91608\n[1800]\ttraining's multi_logloss: 1.84745\tvalid_1's multi_logloss: 1.91595\n[1900]\ttraining's multi_logloss: 1.84387\tvalid_1's multi_logloss: 1.91587\n[2000]\ttraining's multi_logloss: 1.84035\tvalid_1's multi_logloss: 1.91571\n[2100]\ttraining's multi_logloss: 1.83685\tvalid_1's multi_logloss: 1.91559\n[2200]\ttraining's multi_logloss: 1.83346\tvalid_1's multi_logloss: 1.9155\n[2300]\ttraining's multi_logloss: 1.8301\tvalid_1's multi_logloss: 1.91545\n    LightGBM Fold 9 Best Iteration: 2282\n    LightGBM Fold 9 Final Valid Loss (Best Score): 1.9154\n  Fold 10/10\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/sklearn/preprocessing/_label.py:99: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n  y = column_or_1d(y, warn=True)\n/usr/local/lib/python3.11/dist-packages/sklearn/preprocessing/_label.py:134: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n  y = column_or_1d(y, dtype=self.classes_.dtype, warn=True)\n/usr/local/lib/python3.11/dist-packages/sklearn/preprocessing/_label.py:134: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n  y = column_or_1d(y, dtype=self.classes_.dtype, warn=True)\n","output_type":"stream"},{"name":"stdout","text":"[100]\ttraining's multi_logloss: 1.92732\tvalid_1's multi_logloss: 1.93194\n[200]\ttraining's multi_logloss: 1.91885\tvalid_1's multi_logloss: 1.9277\n[300]\ttraining's multi_logloss: 1.9124\tvalid_1's multi_logloss: 1.92551\n[400]\ttraining's multi_logloss: 1.90658\tvalid_1's multi_logloss: 1.92397\n[500]\ttraining's multi_logloss: 1.90125\tvalid_1's multi_logloss: 1.92271\n[600]\ttraining's multi_logloss: 1.89619\tvalid_1's multi_logloss: 1.92175\n[700]\ttraining's multi_logloss: 1.89144\tvalid_1's multi_logloss: 1.92103\n[800]\ttraining's multi_logloss: 1.88696\tvalid_1's multi_logloss: 1.92043\n[900]\ttraining's multi_logloss: 1.88254\tvalid_1's multi_logloss: 1.91995\n[1000]\ttraining's multi_logloss: 1.87828\tvalid_1's multi_logloss: 1.9195\n[1100]\ttraining's multi_logloss: 1.87413\tvalid_1's multi_logloss: 1.9191\n[1200]\ttraining's multi_logloss: 1.87007\tvalid_1's multi_logloss: 1.91873\n[1300]\ttraining's multi_logloss: 1.86606\tvalid_1's multi_logloss: 1.91849\n[1400]\ttraining's multi_logloss: 1.86222\tvalid_1's multi_logloss: 1.91818\n[1500]\ttraining's multi_logloss: 1.85846\tvalid_1's multi_logloss: 1.91796\n[1600]\ttraining's multi_logloss: 1.85475\tvalid_1's multi_logloss: 1.91774\n[1700]\ttraining's multi_logloss: 1.85109\tvalid_1's multi_logloss: 1.91754\n[1800]\ttraining's multi_logloss: 1.84747\tvalid_1's multi_logloss: 1.91741\n[1900]\ttraining's multi_logloss: 1.84397\tvalid_1's multi_logloss: 1.91727\n[2000]\ttraining's multi_logloss: 1.84043\tvalid_1's multi_logloss: 1.91723\n[2100]\ttraining's multi_logloss: 1.83693\tvalid_1's multi_logloss: 1.91722\n    LightGBM Fold 10 Best Iteration: 2031\n    LightGBM Fold 10 Final Valid Loss (Best Score): 1.9172\n\nBase model training complete for all configurations.\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"test_preds","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-01T18:35:38.647618Z","iopub.execute_input":"2025-07-01T18:35:38.647911Z","iopub.status.idle":"2025-07-01T18:35:38.654176Z","shell.execute_reply.started":"2025-07-01T18:35:38.647893Z","shell.execute_reply":"2025-07-01T18:35:38.653610Z"}},"outputs":[{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"{'xgb': array([[0.17704039, 0.10883566, 0.11805922, ..., 0.15576426, 0.17522261,\n         0.11778907],\n        [0.13562677, 0.08898067, 0.25316377, ..., 0.11801846, 0.094445  ,\n         0.15641103],\n        [0.13479239, 0.12679926, 0.13260986, ..., 0.16841967, 0.12499098,\n         0.12070203],\n        ...,\n        [0.15255471, 0.17119473, 0.1258512 , ..., 0.10221327, 0.18734015,\n         0.13161796],\n        [0.23351015, 0.10626551, 0.1479719 , ..., 0.18762837, 0.13454856,\n         0.12081731],\n        [0.16092811, 0.23132355, 0.16813521, ..., 0.10829261, 0.05643388,\n         0.08873593]]),\n 'lgb': array([[0.13660293, 0.12659007, 0.12768463, ..., 0.16593347, 0.16303582,\n         0.13298996],\n        [0.15100126, 0.11534047, 0.20895447, ..., 0.14009263, 0.09200501,\n         0.12340266],\n        [0.14494362, 0.14921054, 0.11588994, ..., 0.15326092, 0.10337601,\n         0.15110053],\n        ...,\n        [0.13530365, 0.18744408, 0.12066773, ..., 0.11022424, 0.17223096,\n         0.15016077],\n        [0.19049222, 0.11147338, 0.18639027, ..., 0.1390149 , 0.15310903,\n         0.13626694],\n        [0.14191657, 0.19577983, 0.19052848, ..., 0.11928382, 0.07681902,\n         0.09783043]])}"},"metadata":{}}],"execution_count":14},{"cell_type":"markdown","source":"## Stacking Ensemble Meta Model: LightGBM","metadata":{}},{"cell_type":"code","source":"stacking_train_feats = np.hstack([oof_preds[name] for name in oof_preds])\nstacking_test_feats = np.hstack([test_preds[name] for name in test_preds])\nmeta_model = LGBMClassifier(objective='multiclass', num_class=y.nunique(), \n                            learning_rate=0.03, n_estimators=1000, \n                            random_state=42, verbose=-1, device='gpu')\nfinal_test_preds = np.zeros((len(X_test), y.nunique()))\nfor fold, (train_idx, valid_idx) in enumerate(skf.split(stacking_train_feats, y)):\n    meta_model.fit(stacking_train_feats[train_idx], y_encoded.iloc[train_idx], \n                   eval_set=[(stacking_train_feats[valid_idx], y_encoded.iloc[valid_idx])], \n                   callbacks=[lgb.early_stopping(100, verbose=False)])\n    final_test_preds += meta_model.predict_proba(stacking_test_feats) / num_of_folds","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-01T18:35:38.654974Z","iopub.execute_input":"2025-07-01T18:35:38.655173Z","iopub.status.idle":"2025-07-01T18:41:43.963546Z","shell.execute_reply.started":"2025-07-01T18:35:38.655157Z","shell.execute_reply":"2025-07-01T18:41:43.962924Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/sklearn/preprocessing/_label.py:99: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n  y = column_or_1d(y, warn=True)\n/usr/local/lib/python3.11/dist-packages/sklearn/preprocessing/_label.py:134: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n  y = column_or_1d(y, dtype=self.classes_.dtype, warn=True)\n/usr/local/lib/python3.11/dist-packages/sklearn/preprocessing/_label.py:134: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n  y = column_or_1d(y, dtype=self.classes_.dtype, warn=True)\n/usr/local/lib/python3.11/dist-packages/sklearn/preprocessing/_label.py:99: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n  y = column_or_1d(y, warn=True)\n/usr/local/lib/python3.11/dist-packages/sklearn/preprocessing/_label.py:134: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n  y = column_or_1d(y, dtype=self.classes_.dtype, warn=True)\n/usr/local/lib/python3.11/dist-packages/sklearn/preprocessing/_label.py:134: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n  y = column_or_1d(y, dtype=self.classes_.dtype, warn=True)\n/usr/local/lib/python3.11/dist-packages/sklearn/preprocessing/_label.py:99: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n  y = column_or_1d(y, warn=True)\n/usr/local/lib/python3.11/dist-packages/sklearn/preprocessing/_label.py:134: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n  y = column_or_1d(y, dtype=self.classes_.dtype, warn=True)\n/usr/local/lib/python3.11/dist-packages/sklearn/preprocessing/_label.py:134: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n  y = column_or_1d(y, dtype=self.classes_.dtype, warn=True)\n/usr/local/lib/python3.11/dist-packages/sklearn/preprocessing/_label.py:99: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n  y = column_or_1d(y, warn=True)\n/usr/local/lib/python3.11/dist-packages/sklearn/preprocessing/_label.py:134: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n  y = column_or_1d(y, dtype=self.classes_.dtype, warn=True)\n/usr/local/lib/python3.11/dist-packages/sklearn/preprocessing/_label.py:134: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n  y = column_or_1d(y, dtype=self.classes_.dtype, warn=True)\n/usr/local/lib/python3.11/dist-packages/sklearn/preprocessing/_label.py:99: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n  y = column_or_1d(y, warn=True)\n/usr/local/lib/python3.11/dist-packages/sklearn/preprocessing/_label.py:134: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n  y = column_or_1d(y, dtype=self.classes_.dtype, warn=True)\n/usr/local/lib/python3.11/dist-packages/sklearn/preprocessing/_label.py:134: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n  y = column_or_1d(y, dtype=self.classes_.dtype, warn=True)\n/usr/local/lib/python3.11/dist-packages/sklearn/preprocessing/_label.py:99: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n  y = column_or_1d(y, warn=True)\n/usr/local/lib/python3.11/dist-packages/sklearn/preprocessing/_label.py:134: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n  y = column_or_1d(y, dtype=self.classes_.dtype, warn=True)\n/usr/local/lib/python3.11/dist-packages/sklearn/preprocessing/_label.py:134: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n  y = column_or_1d(y, dtype=self.classes_.dtype, warn=True)\n/usr/local/lib/python3.11/dist-packages/sklearn/preprocessing/_label.py:99: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n  y = column_or_1d(y, warn=True)\n/usr/local/lib/python3.11/dist-packages/sklearn/preprocessing/_label.py:134: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n  y = column_or_1d(y, dtype=self.classes_.dtype, warn=True)\n/usr/local/lib/python3.11/dist-packages/sklearn/preprocessing/_label.py:134: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n  y = column_or_1d(y, dtype=self.classes_.dtype, warn=True)\n/usr/local/lib/python3.11/dist-packages/sklearn/preprocessing/_label.py:99: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n  y = column_or_1d(y, warn=True)\n/usr/local/lib/python3.11/dist-packages/sklearn/preprocessing/_label.py:134: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n  y = column_or_1d(y, dtype=self.classes_.dtype, warn=True)\n/usr/local/lib/python3.11/dist-packages/sklearn/preprocessing/_label.py:134: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n  y = column_or_1d(y, dtype=self.classes_.dtype, warn=True)\n/usr/local/lib/python3.11/dist-packages/sklearn/preprocessing/_label.py:99: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n  y = column_or_1d(y, warn=True)\n/usr/local/lib/python3.11/dist-packages/sklearn/preprocessing/_label.py:134: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n  y = column_or_1d(y, dtype=self.classes_.dtype, warn=True)\n/usr/local/lib/python3.11/dist-packages/sklearn/preprocessing/_label.py:134: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n  y = column_or_1d(y, dtype=self.classes_.dtype, warn=True)\n/usr/local/lib/python3.11/dist-packages/sklearn/preprocessing/_label.py:99: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n  y = column_or_1d(y, warn=True)\n/usr/local/lib/python3.11/dist-packages/sklearn/preprocessing/_label.py:134: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n  y = column_or_1d(y, dtype=self.classes_.dtype, warn=True)\n/usr/local/lib/python3.11/dist-packages/sklearn/preprocessing/_label.py:134: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n  y = column_or_1d(y, dtype=self.classes_.dtype, warn=True)\n","output_type":"stream"}],"execution_count":15},{"cell_type":"markdown","source":"# Make Prediction","metadata":{}},{"cell_type":"code","source":"top_3_preds_indices = np.argsort(final_test_preds, axis=1)[:, ::-1][:, :3]\nlabel_encoder.fit(train_df['Fertilizer Name'].unique()) # Re-fit on original labels\ntop_3_labels = label_encoder.inverse_transform(top_3_preds_indices.ravel()).reshape(top_3_preds_indices.shape)\nsubmission = pd.DataFrame({\n    'id': test_df['id'],\n    'Fertilizer Name': [' '.join(row) for row in top_3_labels]\n})\nsubmission.to_csv('submission.csv', index=False)\nprint(\"Successfully generated 'submission.csv'\")\nprint(submission.head())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-01T18:41:43.964322Z","iopub.execute_input":"2025-07-01T18:41:43.964581Z","iopub.status.idle":"2025-07-01T18:41:44.646818Z","shell.execute_reply.started":"2025-07-01T18:41:43.964558Z","shell.execute_reply":"2025-07-01T18:41:44.646051Z"}},"outputs":[{"name":"stdout","text":"Successfully generated 'submission.csv'\n       id       Fertilizer Name\n0  750000    DAP 10-26-26 28-28\n1  750001   17-17-17 Urea 20-20\n2  750002  20-20 28-28 10-26-26\n3  750003     14-35-14 DAP Urea\n4  750004   20-20 10-26-26 Urea\n","output_type":"stream"}],"execution_count":16}]}